<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[ParseHub]]></title><description><![CDATA[Thoughts, stories and ideas.]]></description><link>https://www.parsehub.com/blog/</link><generator>Ghost 0.8</generator><lastBuildDate>Mon, 11 Mar 2019 19:08:39 GMT</lastBuildDate><atom:link href="https://www.parsehub.com/blog/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Getting Started in Cryptocurrency Investing]]></title><description><![CDATA[<p>Cryptocurrency - after Bitcoin’s price surge in mid- to late-2016, everyone and their grandmothers know about crypto (seriously, there are memes about it). Since then, however, the crypto market has cooled down a bit. </p>

<p>With the right know-how and some historical price data, you can make quite a bit</p>]]></description><link>https://www.parsehub.com/blog/getting-started-in-cryptocurrency-investing/</link><guid isPermaLink="false">4b56fb93-d65a-4c7a-b18b-2c3f4d7810b3</guid><dc:creator><![CDATA[Melvin Bayoneto]]></dc:creator><pubDate>Fri, 02 Nov 2018 19:07:17 GMT</pubDate><content:encoded><![CDATA[<p>Cryptocurrency - after Bitcoin’s price surge in mid- to late-2016, everyone and their grandmothers know about crypto (seriously, there are memes about it). Since then, however, the crypto market has cooled down a bit. </p>

<p>With the right know-how and some historical price data, you can make quite a bit of money investing in crypto. But how can you obtain the data that you need to make sound investments in this volatile market? And how does a crypto newbie know when the right time to invest is?</p>

<p>I’ll show you how to scrape historical pricing data from CoinMarketCap using ParseHub, as well as show you an easily recognizable pattern that can you use to get started with crypto investing. </p>

<h2 id="projectsetup">Project Set Up</h2>

<p>From the <a href="https://coinmarketcap.com/">main CoinMarketCap page</a>, I am going to <a href="https://help.parsehub.com/hc/en-us/articles/217753378-Select">Select</a> each coin’s name, and then add a <a href="https://help.parsehub.com/hc/en-us/articles/217752908-Click">Click</a> command to go into each coin’s page in order to extract historical price data. I then added Select/Click commands to <a href="https://help.parsehub.com/hc/en-us/articles/217735328-Click-on-the-Next-button-to-scrape-multiple-pages-pagination-">paginate</a> so that I can extract data for every coin listed on this website.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/10/2018-10-30_16-06-17.png" alt="project1"></p>

<p>Next, I used some Select/Click commands to navigate to the historical data tab, and then show the last 3 months of data using the dropdown menu.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/10/2018-10-30_16-14-48.png" alt="project2">
<img src="https://www.parsehub.com/blog/content/images/2018/10/2018-10-30_16-16-49.png" alt="project3"></p>

<p>Now that we have the prices pulled up for the last 3 months, we are going to extract dates, and their open, high, low and close prices <a href="https://help.parsehub.com/hc/en-us/articles/221873968-How-to-scrape-data-from-a-table">from the table</a> so that we can chart them in Excel.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/10/2018-10-30_16-30-33.png" alt="project4"></p>

<h2 id="analysis">Analysis</h2>

<p>Now that we have our data, we can start analyzing it. It is much easier to analyze this data in graphic form, so we are going to use the Open, High, Low, Close graph template that Excel has under the “Waterfall” category. For this example, we are going to graph the pricing data for Noah Coin. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/10/cryptograph-2.png" alt="graph"></p>

<p>Most price rises in cryptocurrency are unsustainable because people tend to sell off their coins when there is a large jump in price, which lowers the price of the coin. So, when the price of a coin goes up, it just as quickly falls back down to a price that is more stable. A coin that has had one rise in price is bound to have another (unless it fails), so the ideal time to invest in a coin is during a period of steady prices that follows a sharp rise and fall. </p>

<p>You can see one of these patterns in the graphs above. On August 29th, there was a large jump in the price of Noah Coin. The price cooled off during the month of September before shooting up again at the beginning of October. This period of stability in mid-September would have been the ideal time to invest in Noah Coin. You can see another period of stability forming at the tail end of the graph.</p>

<p>Of course, there is so much more to cryptocurrency than prices and when to buy/sell. Most cryptocurrencies are built with a specific function in mind that uses the blockchain as a framework. For example, Noah Coin is an alt-coin being used to <a href="https://coinswitch.co/info/noah-coin/what-is-noah-coin">support global, social, and economic development</a>. Not all of these functions are going to be useful or are novel, so some cryptocurrencies are bound to crash. You should read up on the currencies that you plan on investing on to see if they can actually succeed in the marketplace.</p>

<p>This post was written using <a href="https://www.tradingheroes.com/best-cryptocurrency-to-invest-in/#The_Only_Chart_Pattern_You_Need_to_Know_(For_Now)">this strategy guide from Trading Heroes</a> as a guideline. I would highly suggest this guide for anyone wanting to get into the cryptocurrency market. You should also check out our <a href="https://help.parsehub.com/hc/en-us">Help Center</a> for more helpful tutorials that can help you scrape almost any website!</p>

<p>Disclaimer: This article is not an endorsement of Noah Coin. If you decide to invest in cryptocurrencies, please do your own due diligence before doing so.</p>]]></content:encoded></item><item><title><![CDATA[The Conversation about Legalization and its Effects on the Stock Market]]></title><description><![CDATA[<p>As the legalization of recreational marijuana in Canada looms, my Twitter timeline is inundated with tweets about it. Rules and regulations, political discussions, advertisements - if you haven’t heard about this “event” by now, you must be living under a rock. And the value of weed stocks seem to</p>]]></description><link>https://www.parsehub.com/blog/the-conversation-about-legalization/</link><guid isPermaLink="false">7db09fbe-548d-4fd7-86ae-6a5f23e47e0e</guid><category><![CDATA[Web Scraping]]></category><category><![CDATA[Legalization]]></category><category><![CDATA[Legalization in Canada]]></category><dc:creator><![CDATA[Melvin Bayoneto]]></dc:creator><pubDate>Thu, 18 Oct 2018 18:54:12 GMT</pubDate><content:encoded><![CDATA[<p>As the legalization of recreational marijuana in Canada looms, my Twitter timeline is inundated with tweets about it. Rules and regulations, political discussions, advertisements - if you haven’t heard about this “event” by now, you must be living under a rock. And the value of weed stocks seem to be on a sharp increase because of it. But all this talk about legalization got me thinking, is what I’m seeing on my timeline actually what the world is talking about? Is legalization that big of a deal for people? And is the talk about this topic and the increase in the value of stock prices related?</p>

<p>To find out if this is the case, I found a website that lists twitter trends across the world for free. Using #marijuana as a search query, I used ParseHub to scrape the trends’ information including dates, locations, and a few tweets from the trends. I also used ParseHub to scrape stock price information (opening prices and dates) for 3 companies in the marijuana industry. Here is what I found:</p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/10/weedgraph-4.png" alt=""></p>

<p>Unsurprisingly, to me anyway, the majority of the trends associated with this hashtag have to do with legalization. If you look at the content of the tweets for dates with the most trends, they are mostly talking about this topic, with the date with the most trends occurring on February 6th, 2018. This period in time was marked by heated discussions about legalization in the US. </p>

<p>However, what I wasn’t expecting was that the conversation about policy reform/legalization in India would hit the trending page as well. I wasn’t even aware that India was having this conversation until doing research for this blog post. This suggests that talk of decriminalization and legalization is much more widespread than I would have first suspected. </p>

<p>For the most part, the Twitter trends have a very slight positive correlation to the stock prices of companies in the marijuana industry. However, as you pass the date that Bill C-45 was introduced, these numbers seem to be more strongly correlated to each other.</p>

<p>This is not to say, however, that one factor influences the other. What this seems to be a function of is the news coming out about legalization. As legalization comes closer to fruition, more and more news comes out about it every day, and the higher stock prices rise due to the increase in profitability of the industry as a whole. Since this is such a hot button topic, everyone wants to jump in on the conversation whenever news comes out about it. This is why it looks like one factor influences the other.</p>

<p>For example, one of the largest periods of growth that weed stocks saw in this time was the period from November 2017 to January 2018. This was due to <a href="https://www.leafly.ca/news/industry/dows-big-one-day-drop-doesnt-hurt-cannabis-stocks">California’s Adult Use of Marijuana Act (AUMA) going into effect on January 1, 2018</a>. A big deal at the time, it created a lot of buzz on social media about widespread legalization.</p>

<p>In conclusion, what I’m seeing on my Twitter timeline is correct - the legalization of marijuana in Canada is a popular topic on social media right now. In addition, these trends are somewhat related to the stock prices of weed stocks. However, both of these upwards trends seem to be dependant on the news cycle, and not on each other.</p>

<p>Twitter trend data was scraped from <a href="https://www.trendogate.com">trendogate.com</a>, and stock prices were scraped from <a href="https://www.finance.yahoo.com">Yahoo Finance</a>. If you want to try and scrape this data from yourself, visit our <a href="https://help.parsehub.com/hc/en-us">Help Center</a> for some helpful tutorials about <a href="https://help.parsehub.com/hc/en-us/articles/217735328-Click-on-the-Next-button-to-scrape-multiple-pages-pagination-">scraping multiple pages of results</a> or <a href="https://help.parsehub.com/hc/en-us/articles/221873968-How-to-scrape-data-from-a-table">scraping data from a table</a>. If you have any questions about scraping either of these websites, you can email us at hello@parsehub.com!</p>]]></content:encoded></item><item><title><![CDATA[Using Web Scraping to Compare Sneaker Prices]]></title><description><![CDATA[<p>While I was in university, I was introduced to sneaker collecting by one of my floormates in the dorm I was living in. I wouldn’t call myself a “sneakerhead”, but I definitely love rocking a fresh pair of kicks. But one of the worst things about sneaker collecting (as</p>]]></description><link>https://www.parsehub.com/blog/using-parsehub-to-compare-sneaker-prices/</link><guid isPermaLink="false">c4eb1a7e-9c2c-42b0-9217-4c26b4317fdd</guid><category><![CDATA[Web Scraping]]></category><category><![CDATA[Web Scraping Tool]]></category><category><![CDATA[Data Fun]]></category><category><![CDATA[Sneakers]]></category><category><![CDATA[Jordan Brand]]></category><dc:creator><![CDATA[Melvin Bayoneto]]></dc:creator><pubDate>Mon, 25 Jun 2018 18:47:15 GMT</pubDate><content:encoded><![CDATA[<p>While I was in university, I was introduced to sneaker collecting by one of my floormates in the dorm I was living in. I wouldn’t call myself a “sneakerhead”, but I definitely love rocking a fresh pair of kicks. But one of the worst things about sneaker collecting (as a broke college student) are the prices. $750 for a pair of Levi’s 4s? No thanks. I started comparing sneaker prices from popular online sneaker retailers because of this large barrier to entry. I tried doing this by maintaining a spreadsheet by hand, but it was way too time consuming. A web scraper, such as ParseHub, made this process of comparing sneaker prices much easier.  A couple of simple projects and some excel manipulation makes finding the best price for a particular colorway of sneaker is a breeze. I’ll show you how to do it using 2 of the most popular online sneaker retailers.</p>

<p>Website 1: FlightClub.com</p>

<p>Flight Club was the first website I was introduced to. It’s a good source for both deadstock and used sneakers. For this example, I’m using the Jordan 3s, since it’s my favorite Jordan sneaker.</p>

<p>The Flight Club website has a navigation bar, which makes it easy to see deals for the sneakers that you are interested in. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/06/2018-06-22_16-27-45.png" alt="flightclub1"></p>

<p>I started out by selecting all of the elements in this bar and nesting a condition under this selection to look for the Jordan category. I did this so that you can easily change this command to look for any category that you are interested in. After setting it to hover over this element and waiting for the menu to appear, I set another condition to look for the Jordan 3 link and click on it once found.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/06/2018-06-22_16-28-46.png" alt="flightclub2"></p>

<p>The landing page for the listings has a starting price under each sneaker, but I wanted to find more details. So, I set ParseHub to click on each listing page, and then set it to go to the next page once it’s done scraping the listings from this first page. This webpage has a problem with an infinite loop, so I set a condition looking for an “inactive” class on the next button. If the button is inactive, ParseHub will not click on the next button.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/06/2018-06-22_16-51-44.png" alt="flightclub3"></p>

<p>Once on the page of the actual listing, I set ParseHub to select and extract the colorway of the sneaker, and then trained it onto the sizes section. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/06/2018-06-22_17-05-03.png" alt="flightclub4"></p>

<p>I set it to click on my size using a condition (10.5) and the scrape the resulting price of a new pair. I followed the same process for a used pair as well.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/06/2018-06-22_17-06-44.png" alt="flightclub5"></p>

<p>Website 2: StadiumGoods.com</p>

<p>I find that Stadium Goods has a wider selection of merchandise, so it’s usually my go-to website for looking up sneaker prices. Again, I’m just going to search for Jordan 3s, but you can find pretty much any sneaker you want on their website.</p>

<p>The majority of my set up for this project was the same as my Flight Club project, but there were a few differences.</p>

<p>The listing pages have both colorway and nickname fields on them. Since I know most of these colorways by their nicknames, I set ParseHub to extract both the nicknames and colorways (just in case a colorway didn’t have a nickname).</p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/06/2018-06-22_17-10-07.png" alt="stadiumgoods1"></p>

<p>In addition, the sizes on Stadium Goods are in a dropdown menu. I set ParseHub to select and click on the dropdown menu, then did the same search for my size that I did on Flight Club.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/06/2018-06-22_17-11-00.png" alt="stadiumgoods2"></p>

<p>Now that all of the data is gathered, it’s time for some Excel. First, I transferred both data sets to a clean Excel spreadsheet. The colorways from Flight Club were a little messy, so I did some simple find + replace to isolate the colorways.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/06/2018-06-22_17-16-19.png" alt="excel1"></p>

<p>I then set up a simple formula to look for certain colorway (Katrina, in this example). If found, it will return the price of the shoe.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/06/2018-06-22_17-18-57.png" alt="excel2"></p>

<p>After that, it’s just a matter of looking up the listing page that you are interested in. You can also set ParseHub to extract the URLs of the listing pages to make finding them even easier! This project should be easy to recreate using this article, but if you have any questions about your own project, feel free to contact us at hello@parsehub.com</p>]]></content:encoded></item><item><title><![CDATA[How to lose your self esteem in 5 easy steps: The story of a woman in tech]]></title><description><![CDATA[<p>Hey there prospective computer science student. I’m assuming you’re excited to get started. I have just completed my fourth year as a computer science student and I believe I’m well suited to give you advice about the journey you are soon to begin. </p>

<p>I’ll start with</p>]]></description><link>https://www.parsehub.com/blog/how-to-lose-your-self-esteem-in-5-easy-steps-the-story-of-a-woman-in-tech/</link><guid isPermaLink="false">4dbeae64-49fc-431f-aa93-856645ed9b94</guid><dc:creator><![CDATA[Taamannae Taabassum]]></dc:creator><pubDate>Thu, 31 May 2018 21:32:40 GMT</pubDate><media:content url="https://www.parsehub.com/blog/content/images/2018/05/cow.png" medium="image"/><content:encoded><![CDATA[<img src="https://www.parsehub.com/blog/content/images/2018/05/cow.png" alt="How to lose your self esteem in 5 easy steps: The story of a woman in tech"><p>Hey there prospective computer science student. I’m assuming you’re excited to get started. I have just completed my fourth year as a computer science student and I believe I’m well suited to give you advice about the journey you are soon to begin. </p>

<p>I’ll start with this: there are no average female developers. That’s a weird thing to say, but it encapsulates my experience as a woman in tech. You are either the best of the best or you’re not seen as worthy of being in the field. For the past few years it has been easy for me to feel like I am not allowed to make a mistake, lest I be called unfit for a position in a field I worked so hard to get into. In contrast to my experience as a developer, I am also a designer but the standards for designers are not the same for women developers. There can be average designers. I can create stunning visual images with ease, but minor mistakes never made others question my skill set as a whole. It’s almost as if the people I was interacting with, my peers at school, new individuals I met, and people I was working with professionally had expectations that I would be a designer. It made sense that as a woman, I would be working in some sort of art field. I lost self worth and sense of accomplishment, I felt as if I was a fraud trying to convince people I wasn’t. </p>

<p>I didn’t always feel this way about being a woman in tech. My slow descent into the black hole that is imposter syndrome started on a pretty happy note. </p>

<h1>Step 1: Actually enjoy the field</h1>

<p>I started my journey into the tech world at the age of 10. The online game called Neopets let me dip my toes in the field and gave me a chance to explore the field in a fun and challenging way. I got my first copy of Adobe Photoshop and got to work making layouts using the HTML/CSS guidelines on the Neopets website. I knew web development was fun for me  because even after my interest in Neopets died down, I would keep making websites and fan sites for other things I enjoyed.</p>

<h1>Step 2: Have your childhood geared to a career in literally anything else</h1>

<p>Web development was a hobby for me in childhood, but it wasn’t a career path. I used to build little sites throughout high school just for fun but I never thought it could be a career option. That’s not to say I didn’t know there were computer scientists, I didn’t even know that what I was doing counted as software development. My knowledge about the breadth of career options was limited by my parents. They wanted me to be a doctor. My whole childhood was built around me eventually becoming a doctor. My dad came to canada in the hopes of getting me into the University of Toronto so I could study medicine and because I was totally clueless, I decided to follow their direction and took biology, physics, chemistry and calculus in high school. The thought of any other potential career was foreign to me and completely forbidden to discuss with my parents. I just accepted my fate, took the biology courses and kept web development as a hobby.</p>

<p>My high school, which was an all girl school, offered a wide variety of art courses such as visual art, drama, dance, and creative writing. The fact that the majority of the elective courses were in the arts field may be perpetuating the ‘women tend towards the arts’ stereotype simply because it was an all girls school. The all boys school that was down the street from us offered robotics, computer programming and construction engineering as electives. Despite the distinct lack of technology related courses at my school, I was fortunate to be able to develop my visual skills in two communication technology courses that they did offer. I currently have a few male friends who started in computer science on Neopets just as I did, but I was the only one who didn’t actually pursue it. It seemed like the natural progression from HTML/CSS would be to make complete software systems as a frontend engineer but I didn't make that leap. I never moved on to being a "front end developer" and learning Javascript because I'd hit the societal glass ceiling of "women are designers, men are the rest" '</p>

<p>This cluelessness followed me into university because I applied for only life science majors in various universities, thinking I was going to love it.</p>

<p>Boy, was I wrong.</p>

<p>I hated my first year, I wasn’t doing well and I disliked a majority of what I was learning. It was boring. Luckily, I met someone who was studying in computer science and I’d sit in some of his classes and I loved it. It was then did I realize that this was where I truly wanted to be, so I made the switch. </p>

<h1>Step 3:  Have peers and “supportive” communities put you down</h1>

<p>In my second year, I took my first computer science class and learned my first real programming language, Python. The logical approach to solving problems in my programming course was a stark contrast to how things were done in life science. I found myself relating my daily life to what I was learning. I was seeing binary trees everywhere and doing recursion problems in my head on the commute home from school. For the first time, I truly felt like I belonged somewhere, I was doing significantly better and enjoying what I was learning. My web design skills improved after I learned Javascript and it blew my mind. Things I wanted to do as a child with my HTML/CSS site were made so much easier with Javascript and I wished that I had learned it before. </p>

<p>Things started changing for the worse. In my transition between second and third  year when I started having to rely on partners for projects. That was when I truly had to depend on my peers and for the first time, I was put into the ‘women are bad at tech’ category. I’d heard that everyone has some horrible experience with at least one group project and this for me was no exception. My posts asking for a partner on class discussion groups like Piazza would almost never got replies ,and when I did end up with a partner, they weren't exactly kind. I had to resort to asking random people in class to be my partner. I would pitch ideas and be met with variations of  “No, your idea does not work”. Even trying to explain what my thinking process is would be met with endless streams of “No, that is not the answer, we are not doing that.” Then, after all of inter-team discussion, I’d still end up doing most of the work. </p>

<p>Even outside of pure academics, there were toxic communities within University itself. The Computer Science Student Union, claimed their mission was to make it a safe and equal place, but actually did the opposite. Walking into the room was almost suffocating. While words flowed from opinionated mouths, I found no place for myself. At one point, I walked in on two guys talking about why there weren’t a lot of women in tech. I sat there baffled, listening to them spout how it’s because woman just don’t want to because they’d rather go into women dominated field like nursing. I wanted to hear what they had to say about the issue since they were expressing their opinions so boldly. I finally built the courage to ask “Have you asked an actual woman why there aren’t women in tech?” They sat in stunned silence. The thought to ask a woman about a topic pertaining to women had not even crossed their minds. </p>

<h1>Step 4:  Feel so bad about yourself that you want to want to escape the situation entirely </h1>

<p>This experience completely ruined my self esteem. No longer was I the 10 year old who enjoyed drawing things on Photoshop and making websites. Suddenly, my self worth became tied to my ability to do computer science tasks. I felt like I was no longer allowed to be wrong in fear that someone would see me as incapable. For most of those years, I suffered through imposter syndrome even though I was constantly given proof that I was thriving. It made me feel even more intimidated by men in tech, especially those who knew what they were doing. Making a mistake in front of them seemed like the end of the world because I’d be seen as an imposter, undeserving of the position. I still struggle with seeing myself as someone who has worked hard and it is something I am trying to actively work on because I know I am capable. I am trying to separate my skill set from my worth as a person and see that mistakes don’t make me totally incapable, even though it is what I’ve been led to believe for a large part of my adult life. </p>

<p>I feel that I lost that excitement after taking my first computer science class. Instead, I just wanted school to be over, I wanted to escape the confines of people’s views of me. Part of me was just trying to graduate just so I could do something else and learn in a different environment. There had to be some sort of change.  </p>

<h1>Step 5: Getting out of there and moving up and on</h1>

<p>I knew my escape was somewhere outside of university, somewhere that people were open and saw skills as being outside of the person.</p>

<p>Part of this change came from joining ParseHub. I accepted a position at ParseHub after listening to the leaders of the company talking about their views. I read a handbook before I even interviewed that made me feel better about what I was getting into. Despite that, I still came to work very afraid that I was incapable and was instantly intimidated by the strong male presence. People knew what they were talking about and were not afraid to actually talk about it. Since my start date, I have worked with many intelligent and kind individuals who didn’t see me as less of a person for not knowing something, but rather had already separated my worth from my skill set. The ParseHub policy to ask for help if you’re stuck for more than five minutes allowed me to feel comfortable in not knowing. If I expressed that I felt incapable, I was met with rounds of “We will catch you up” or “Here is how we do what you’re confused about”. For once, I felt more like an equal. To be completely honest, I still feel very intimidated by men who know a lot but it’s something I’m striving to fix. I am always asked for my opinion on company policy and encouraged to point out any flaws in the system. Everyone has always been open minded and accepting of ideas and different views. </p>

<p>Perhaps my fear came from a mixture of my own experience and hearing about the experience of other women in the field (and there is a lot of it). it is a reality for many women in tech. It is so easy to feel like you don’t belong, especially when people around you don’t do anything to remedy the situation and make you feel like you really don’t belong. </p>

<p>My story might seem very scary and intimidating, especially for someone who is considering joining the computer science community, but the intention is not to scare prospective computer scientists off. My story’s purpose is to show that while things may seem really hard and scary and it might feel like the whole world is fighting to put you down, there is always light at the end of the tunnel. Despite the hardship I faced and continue to face, my graduation is fast approaching and I am working at an amazing company that values my opinion and teaches me when I am wrong instead of cutting me out.</p>]]></content:encoded></item><item><title><![CDATA[A Complete Guide on Offering Web Scraping Services]]></title><description><![CDATA[<h2>What is web scraping?</h2>

<p>Web scraping is a term used for collecting information from websites on the internet. </p>

<p>Clients who are interested in web scraping services will typically have a goal in mind like "I want all names, phone numbers, addresses and email addresses for every contact on this directory"</p>]]></description><link>https://www.parsehub.com/blog/a-complete-guide-on-offering-web-scraping-services/</link><guid isPermaLink="false">8659fd97-dabc-4e43-b6db-abe5ebd67be2</guid><category><![CDATA[Web Scraping]]></category><category><![CDATA[Web Scraping Tool]]></category><category><![CDATA[Freelancers]]></category><category><![CDATA[Web Scraping Services]]></category><dc:creator><![CDATA[Stephanie O'Gay Garcia]]></dc:creator><pubDate>Mon, 07 May 2018 20:37:06 GMT</pubDate><media:content url="https://www.parsehub.com/blog/content/images/2018/05/FreelanceWebScraping.jpg" medium="image"/><content:encoded><![CDATA[<h2>What is web scraping?</h2>

<img src="https://www.parsehub.com/blog/content/images/2018/05/FreelanceWebScraping.jpg" alt="A Complete Guide on Offering Web Scraping Services"><p>Web scraping is a term used for collecting information from websites on the internet. </p>

<p>Clients who are interested in web scraping services will typically have a goal in mind like "I want all names, phone numbers, addresses and email addresses for every contact on this directory" and the scraper will crawl that website to extract that information and export it to the desired format (Excel, JSON, XML... etc.). </p>

<h2>What do people use scraping for?</h2>

<p>There are many reasons people scrape data from the website. Here are some of the most common use cases that we have found: </p>

<ul>
<li>Scraping <b>lead information</b> from directories: either individual contact information or company information to populate CRMs</li>
<li><b>Market research</b>: scrape pricing and other information on products from eCommerce websites, vehicles on dealership sites, trips on travel sites or property information from real estate sites</li>
<li>Collect data from sites for <b>various research purposes</b></li>
<li>Build <b>aggregators</b> that collect blog posts, classified ads or jobs</li>
<li>Scrape data from an old website to <b>move the content over to a new website</b>, where export or API are not available</li>
<li>Scrape <b>stock or cryptocurrency rates</b> regularly</li>
<li>Scrape reviews and comments for <b>sentiment analysis</b></li>
</ul>

<h2>How can I scrape data from the web?</h2>

<p>There are different ways of scraping information: </p>

<ul>
<li><b>Custom web scrapers:</b> these are scrapers built by programmers in a variety of programming languages</li>
<li><b>Web scraping software:</b> these are tools that allow you to scrape data from the web without any information required. </li>
</ul>

<h3>Custom Web Scrapers</h3>

<p>These are typically built by programmers in a variety of languages. People commonly use libraries like <a href="https://parsehub.com/blog/parsehub-vs-scrapy-comparison-which-alternative-is-better-for-web-scraping/" target="_blank">Scrapy</a>, Beautiful Soup and Selenium to build them. </p>

<p>Pros:</p>

<ul>
<li>Highly customizable and tailored to your needs</li>
<li>If you hire someone to build it, little time investment on your part</li>
</ul>

<p>Cons:</p>

<ul>
<li>Difficult to maintain without programming knowledge</li>
<li>If you have hired someone, you need to contact and pay them each time an issue arises or a change is required</li>
<li>Each website requires that an entirely new scraper be built for it</li>
</ul>

<h3>Web Scraping Software</h3>

<p>There are many software companies out there that provide software that allow you to scrape data without any programming knowledge. Some examples include: <a href="https://parsehub.com/blog/parsehub-vs-import-io-which-alternative-is-better-for-web-scraping/" target="_blank">Import.io</a>, Diffbot, <a href="https://parsehub.com/blog/portia-vs-parsehub-comparison-which-alternative-is-the-best-option-for-web-scraping/" target="_blank">Portia</a> and our own software, ParseHub.</p>

<p>Pros: </p>

<ul>
<li>Users can typically set up web scrapers with little or no technical knowledge and are provided with support</li>
<li>Users can also maintain their project without having to contact a developer</li>
<li>Pricing can start off quite low, with many providers offering a free version</li>
</ul>

<p>Cons: </p>

<ul>
<li>Some web scraping software cannot handle more complex sites</li>
<li>You will need to invest time into learning the software</li>
<li>In some cases, you will need to be more technical to use more advanced features</li>
</ul>

<h2>How popular is web scraping?</h2>

<p>The demand for web scraping services is high. </p>

<p>A search for "web scraping" on Upwork shows that there are currently 833 jobs and Freelancer.com shows 1129. </p>

<p>We scraped the <a href="https://www.freelancer.ca/job/" target="_blank">Freelancer.com "Websites, IT &amp; Software" category</a> and, of the 477 skills listed, "Web scraping" was in 21st position. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/05/Screen-Shot-2018-05-07-at-4-05-08-PM.png" alt="A Complete Guide on Offering Web Scraping Services"></p>

<p><center><i>List of skills in the "Websites, IT &amp; Software" category on Freelancer.com in May 2018</i></center></p>

<h2>How much do people usually charge for web scraping services?</h2>

<p>The cost for scraping a website varies, with some online freelancers offering extremely low prices such as $10/website. </p>

<p>However, scraping companies will tend to charge a higher price. We contacted several scraping companies with a quote request for a weekly scrape of 6000 products on Amazon in four categories to extract the title, price, brand, description, ASIN, rating, number of reviews and the "Sold by" name and URL. These are some of the quotes I received: </p>

<ul>
<li><b>$400 initial setup</b> and $4500 - $5000 USD per year for managed services (assuming a middle ground of $4750, this is <b>$395/month maintenance</b>)</li>
<li><b>$99 initial setup</b>, $79/month for monthly maintenance and $5 per 10000 records per month (assuming 6000 records per week, this adds on $12 per month for a total of <b>$91/month maintenance</b>)</li>
<li><b>$149 initial setup</b> and <b>$100/month maintenance</b></li>
<li><b>$329 initial setup</b> and the first 10,000 lines of data. After that, each additional record will be charged at $0.005/line. At 24,000 products per month, this is about <b>$120/month maintenance</b></li>
</ul>

<p>While it's a small sample, it is about a <b>$244 average for the initial setup</b> and a <b>$177 average monthly maintenance fee</b>. These prices are more or less in line with <a href="http://scraping.pro/choosing-web-scraping-service/" target="_blank">those reported by scraping.pro</a>. Not too shabby if the project isn't too complex! </p>

<p>While I did not request other quotes, it's possible that users may simply want a one-time scrape rather than scraping the site on a regular basis. <a href="https://help.parsehub.com/hc/en-us/articles/115005070133-Can-ParseHub-build-my-project-for-me-Web-Scraping-Services-" target="_blank">We provide this service at a cost of $699/website</a>, for example.</p>

<p><a href="https://help.parsehub.com/hc/en-us/articles/360000822593-Video-Tutorial-Scraping-product-details-from-e-commerce-websites-Amazon-example-" target="_blank"><img src="https://www.parsehub.com/blog/content/images/2018/05/ScrapeAmazonVideo.png" alt="A Complete Guide on Offering Web Scraping Services" title=""></a></p>

<h2>How difficult is it to provide web scraping services?</h2>

<p>That depends! It depends on both the method you use (programming vs software) and the complexity of the website. </p>

<p>The above Amazon project, for example, takes an experienced user about 10 - 15 minutes to build and test on ParseHub. If you are not familiar with the software, it may take longer. </p>

<h3>What questions should I ask a potential client?</h3>

<p>In order to understand the complexity of a web scraping project, typically we ask: </p>

<ul>
<li>What website are they trying to scrape? </li>
<li>On that website, what specific elements are they interested in? </li>
<li>What format would they like the data to be extracted to and how would they like that data formatted?</li>
<li>Approximately how many pages will they be scraping?</li>
<li>How regularly do they require this data?</li>
</ul>

<p>On receiving this information we will look at the website and quickly build a sample project to understand how complex it will be. Things to consider are: how structured is the data? Is the layout they are requesting possible?</p>

<p>If the client is scraping a high volume of pages frequently, there may also be issues with the website attempting to block that traffic. In this case, rotating proxies (either from a pool of proxies or using custom proxies for that client) will usually be a requirement. </p>

<h2>Is web scraping legal?</h2>

<p>Web scraping is legal in most cases. While we cannot provide legal expertise, we would encourage you to read some of the following literature and always check the terms of service of the website you are scraping.</p>

<ul>
<li><a href="https://www.import.io/post/dear-canada-accessing-publicly-available-information-on-the-internet-is-not-a-crime/" target="_blank">Dear Canada: Accessing Publicly Available Information on the Internet Is Not a Crime</a></li>
<li><a href="https://www.wsj.com/articles/judge-orders-linkedin-to-allow-startup-access-to-user-data-1502752182" target="_blank">Judge Orders LinkedIn to Allow Startup Access to User Data</a></li>
</ul>

<h2>Final words</h2>

<p>In summary, web scraping is a highly in-demand skill that you can learn with relative ease. It is a great opportunity for agencies, consultants and freelancers to add web scraping to their service line-up.</p>

<p><a href="https://www.parsehub.com/partners" target="_blank"><img src="https://www.parsehub.com/blog/content/images/2018/05/Partner.png" alt="A Complete Guide on Offering Web Scraping Services" title=""></a></p>]]></content:encoded></item><item><title><![CDATA[Toronto Restaurants by Category: Scrape Map Data from a Directory]]></title><description><![CDATA[<p>We often receive requests asking how to <a href="https://help.parsehub.com/hc/en-us/articles/221759968-Scrape-data-from-an-interactive-map" target="_blank">scrape and work with data extracted from a map</a>. For example, consultants, analysts and marketers who are interested in scraping business or store locations from a map for market research or business planning.</p>

<p>Being a foodie who is relatively new to Toronto, I</p>]]></description><link>https://www.parsehub.com/blog/toronto-restaurants-by-category-scrape-map-data-from-a-directory/</link><guid isPermaLink="false">1cfc1a20-6d51-4d60-803d-c22f56235a4f</guid><category><![CDATA[Web Scraping]]></category><category><![CDATA[Web Scraping Tool]]></category><category><![CDATA[location data]]></category><category><![CDATA[map data]]></category><category><![CDATA[Business Hacks]]></category><category><![CDATA[Data Fun]]></category><dc:creator><![CDATA[Stephanie O'Gay Garcia]]></dc:creator><pubDate>Mon, 30 Apr 2018 14:59:00 GMT</pubDate><media:content url="https://www.parsehub.com/blog/content/images/2018/04/YongeStreet-1.png" medium="image"/><content:encoded><![CDATA[<img src="https://www.parsehub.com/blog/content/images/2018/04/YongeStreet-1.png" alt="Toronto Restaurants by Category: Scrape Map Data from a Directory"><p>We often receive requests asking how to <a href="https://help.parsehub.com/hc/en-us/articles/221759968-Scrape-data-from-an-interactive-map" target="_blank">scrape and work with data extracted from a map</a>. For example, consultants, analysts and marketers who are interested in scraping business or store locations from a map for market research or business planning.</p>

<p>Being a foodie who is relatively new to Toronto, I was interested in looking into where restaurants are located by food type. To do so, I scraped the latitude and longitude from all Toronto Restaurants on the Yelp directory and plotted them on to a Tableau Public map which can be filtered by the restaurant’s category.</p>

<p><a href="https://public.tableau.com/profile/stephanie.o.gay.garcia#!/vizhome/TorontoRestaurants/TorontoRestaurantsByCategory" target="_blank"> <br>
<img src="https://www.parsehub.com/blog/content/images/2018/05/MapofTorontoRestaurantsbyCategory.png" alt="Toronto Restaurants by Category: Scrape Map Data from a Directory">
</a></p>

<p><center><i>Click to view the interactive map of Toronto restaurants by category</i></center></p>

<p>The map allows you to select one or more categories on the right-hand sidebar to view the location of Toronto restaurants in those categories (use Shift or Ctrl/Command to select multiple). Clicking into each restaurant on the map will provide you with more information. </p>

<h2>How did I do it?</h2>

<h3>The data</h3>

<p>The data extracted is from <a href="https://www.yelp.ca/c/toronto/restaurants" target="_blank">Yelp’s Toronto Restaurant Categories page</a>. My project loops through every category on the page, clicks into each one and extracts all the restaurants that appear, extracting their latitude, longitude, number of reviews, rating, address, phone, restaurant and price range. </p>

<p>Exact details on how to do this are provided at the end of the article.</p>

<p><center><b><i>Interested in learning more about ParseHub? Book a demo call with us <a href="https://calendly.com/parsehub-product-demo" target="_blank">here</a>.</i></b></center></p>

<h3>Creating the map</h3>

<p>I considered several alternatives on how to plot my map data, such as <a href="https://developers.google.com/maps/" target="_blank">the Google Maps API</a>, but ultimately decided on <a href="https://public.tableau.com/en-us/s/" target="_blank">Tableau Public</a> who offer a free version of their tool. This method was surprisingly easy – I just had to import my CSV/Excel file and specify which columns were the latitude and longitude. </p>

<p><a href="http://kb.tableau.com/articles/howto/plotting-geographic-data-using-custom-longitude-and-latitude" target="_blank">This article</a> explains how to do so in more detail. </p>

<h2>What did I find?</h2>

<p>Toronto is known for its multiculturalism, with some like <a href="https://www.bbc.co.uk/programmes/p03v1r1p" target="_blank">the BBC Radio referring to it as the most diverse city in the world</a>. Over half the population was born outside of Canada and come from 232 different nationalities. The 132 restaurant categories on Yelp are a reflection of the various cultures that share the city.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/05/YongeStreet.png" alt="Toronto Restaurants by Category: Scrape Map Data from a Directory"></p>

<h3>Ethnic Neighbourhoods</h3>

<p>According to <a href="https://en.wikipedia.org/wiki/List_of_neighbourhoods_in_Toronto" target="_blank">Wikipedia’s List of Neighbourhoods in Toronto</a>, there are several neighbourhoods that correspond to different communities such as Little India, Little Italy, Little Portugal, Little Tibet, Greektown, Chinatown, Koreatown and Little Jamaica. I was curious to see if the majority of restaurants from these cultures were found in their respective areas so I searched for the perimeter for each neighbourhood to compare with the restaurants on my map. </p>

<p>Some restaurant types can be found in most neighbourhoods:  </p>

<ul><li>While Indian food is widespread throughout the city, there does appear to be a cluster of restaurants in <a href="http://www.seetorontonow.com/explore-toronto/neighbourhoods/little-india/#sm.0001cqzdeus65faezbv21ga7b82vy" target="_blank">the area See Toronto describes as Little India</a> - on Gerrard Street East, between Coxwell Avenue and Greenwood Avenue.</li>  
<li>Similar to Indian food, Italian food is also widespread but it is true that there is a small cluster in <a href="https://www.google.ca/maps/place/Little+Italy,+Toronto,+ON/@43.6552388,-79.4189599,15.19z/" target="_blank">the area defined as Little Italy</a>.</li></ul>

<p><img src="https://www.parsehub.com/blog/content/images/2018/05/IndianAndItalian.png" alt="Toronto Restaurants by Category: Scrape Map Data from a Directory"></p>

<p><center><i>Maps of Indian restaurants with Little India highlighted (left) and Italian restaurants with Little Italy highlighted (right)</i></center></p>

<p>Some restaurant types are indeed clustered mostly around their ethnic neighbourhood:  </p>

<ul><li>The majority of Portuguese restaurants are mostly around <a href="https://www.google.ca/maps/place/Little+Portugal,+Toronto,+ON/" target="_blank">Toronto’s Little Portugal</a> neighbourhood and surrounding areas.</li>  
<li>There are not too many Tibetan restaurants in the city, but there is a cluster of quite a few at the far west of Queen Street West.</li></ul>

<p><img src="https://www.parsehub.com/blog/content/images/2018/05/PortugueseAndTibetan.png" alt="Toronto Restaurants by Category: Scrape Map Data from a Directory"></p>

<p><center><i>Maps of Portuguese restaurants with an approximation of Little Portugal highlighted (left) and Tibetan restaurants with Little Tibet highlighted (right)</i></center></p>

<p>Some have clusters in different areas:  </p>

<ul><li>There is a cluster of Greek restaurants in Greektown on the Danforth, but there also appears to be quite a few in the downtown Financial District.</li>  
<li>Chinese restaurants are widespread throughout the city. There are clusters in the downtown area around Chinatown, at the north of Yonge Street and also in the north-east in Markham.</li>  
<li>Korean restaurants are largely found downtown with a cluster on Bloor Street West in Koreatown and another at the north of Yonge Street.</li></ul>

<p><img src="https://www.parsehub.com/blog/content/images/2018/05/GreekChineseKorean.png" alt="Toronto Restaurants by Category: Scrape Map Data from a Directory"></p>

<p><center><i>Maps of Greek restaurants with Greektown highlighted (left), Chinese restaurants with Chinatown highlighted (centre) and Korean restaurants with Koreatown highlighted (right)</i></center></p>

<p>Some didn’t appear to have any specific clusters:  </p>

<ul><li>While there are plenty of Caribbean restaurants in the Little Jamaica area, there are also plenty downtown and in the east of the city. There are very few on Yonge Street and its surrounding areas outside of the downtown core.</li></ul>

<p><img src="https://www.parsehub.com/blog/content/images/2018/05/LittleJamaica.png" alt="Toronto Restaurants by Category: Scrape Map Data from a Directory"></p>

<p><center><i>Maps of Jamaican restaurants with an Little Jamaica highlighted</i></center></p>

<p>I also noticed that some restaurants from other cultures are primarily located in specific areas:  </p>

<ul><li>There is a cluster of Ethiopian restaurants on the Danforth</li>  
<li>There are plenty of Tapas and Spanish restaurants just west of downtown</li>  
<li>Running along the Humber River and on the West end of the city are plenty of South American cuisines</li></ul>

<p><img src="https://www.parsehub.com/blog/content/images/2018/05/EthiopianSpanishLatinAmerican.png" alt="Toronto Restaurants by Category: Scrape Map Data from a Directory"></p>

<p><center><i>Maps of Ethiopian restaurants (left), Spanish and Tapas restaurants (centre) and Latin American restaurants - Argentine, Brazilian, Colombian, Latin American, Peruvian and Salvadoran (right)</i></center></p>

<h3>Financial District</h3>

<p>While clicking through the different categories, I noticed a distinct cluttering of Sandwich, Cafes, Soup and Salad around <a href="http://torontofinancialdistrict.com/" target="_blank">the Financial District</a>. These are likely the types of restaurants that people lunch at during their workdays.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/05/SandwichesSaladsSoupCafes.png" alt="Toronto Restaurants by Category: Scrape Map Data from a Directory"></p>

<p><center><i>Sandwiches, Salads, Soups and Cafes are popular in the downtown area</i></center></p>

<h3>The Waffle Mile</h3>

<p>Did anyone know we have a Waffle Mile? I could not help but notice that there is a path of Waffle restaurants downtown. It’s probably more than a mile, but worth every step!</p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/05/WaffleMile.png" alt="Toronto Restaurants by Category: Scrape Map Data from a Directory"></p>

<p><center><i>The Waffle Mile - Waffle restaurants are dotted in green</i></center></p>

<h3>Vegandale?</h3>

<p>Recently there have been news about vegan restaurants in Parkdale attempting to refer to it as <a href="https://www.thestar.com/news/gta/2018/03/27/vegandale-neighbourhood-brings-veganism-to-the-masses.html" target="_blank">Vegandale</a>, but where are all the vegan restaurants? </p>

<p>They do appear to be mostly spread throughout the western side of downtown although Parkdale itself does not necessarily have more vegan restaurants. That being said, the naming also comes from upcoming restaurants that do not currently appear on the map. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2018/05/Vegandale.png" alt="Toronto Restaurants by Category: Scrape Map Data from a Directory"></p>

<p><center><i>Maps of Vegan restaurants with an Parkdale highlighted</i></center></p>

<p>There are plenty of other interesting restaurant category trends you can find on the map, play around with it and let us know in the comments if you’ve spotted any!</p>

<h2>Details on building the project</h2>

<p>If you want to replicate the project that I built on ParseHub, you can do the following. </p>

<ol>
<li><p>Open <a href="https://www.parsehub.com/quickstart" target="_blank">the ParseHub client</a> and click on <b>New Project</b></p></li>
<li><p>Input <a href="https://www.yelp.ca/c/toronto/restaurants">https://www.yelp.ca/c/toronto/restaurants</a> as the URL and click on <b>Start project on this URL</b>  </p></li>
<li><p>Click on the first category (e.g. Afghan) and continue to click on categories until all are selected. You can rename that <b>selection1</b> to Category. </p></li>
<li><p>I wanted to extract the category’s code. To do so, I took the URL Extract command and used RegEx to extract that category. Rename the selection to code. I used the following <a href="https://help.parsehub.com/hc/en-us/articles/217736078-Regular-Expressions-RegEx-" target="_blank">RegEx</a>: <code>https://www.yelp.ca/c/toronto/(.*)</code></p></li>
</ol>

<p><img src="https://www.parsehub.com/blog/content/images/2018/05/Screen-Shot-2018-04-30-at-11-07-29-AM.png" alt="Toronto Restaurants by Category: Scrape Map Data from a Directory"></p>

<ol>
<li>Click on the + sign next to <b>Begin new entry in Category</b> and choose a <b>Go to Template</b> command. For the URL, we want it to go to the Yelp link searching for the category code we extracted previously, as follows: <code>'https://www.yelp.ca/search?cflt=' + code + '&amp;find_loc=Toronto%2C+ON%2C+Canada'</code>. It should go to a new template "category".</li>
</ol>

<p><img src="https://www.parsehub.com/blog/content/images/2018/05/Screen-Shot-2018-04-30-at-11-08-57-AM-1.png" alt="Toronto Restaurants by Category: Scrape Map Data from a Directory"></p>

<ol>
<li><p>A category will open along with the new category template. From here, you can follow the instructions on <a href="https://help.parsehub.com/hc/en-us/articles/115004991414-Scraping-directories-Yelp-Example-" target="_blank">this tutorial</a> to scrape all of the restaurants in that category. </p></li>
<li><p>On the details template, we used a <b>Select</b> command to select the map that appears and then added two <b>Extract</b> commands below it, each extracting the Image URL from the drop-down and using the RegEx <code>center=(-?[\d.]<em>)</em></code> for the latitude and the RegEx <code>center=[-?\d.]%2C([-?\d.]*)</code> for the longitude.</p></li>
</ol>

<p>You should be able to easily replicate this and similar projects using <a href="https://www.parsehub.com/" target="_blank">ParseHub</a>. If you have any questions while you build your own project, you can always contact us at <a href="mailto:hello@parsehub.com" target="_top">hello@parsehub.com</a> or book a demo call <a href="https://calendly.com/parsehub-product-demo" target="_blank">here</a> and we would be happy to assist. </p>

<p>For information on how to scrape data from a store locator, check out <a href="https://www.parsehub.com/blog/how-to-get-the-locations-of-retail-stores-with-web-scraping/" target="_blank">this blog post</a>.</p>]]></content:encoded></item><item><title><![CDATA[Portia vs. ParseHub comparison: which alternative is the best option for web scraping?]]></title><description><![CDATA[Putting Portia and ParseHub head to head. Comparing the features, usability, and cost of using both web scraping tools. Includes an example project.]]></description><link>https://www.parsehub.com/blog/portia-vs-parsehub-comparison-which-alternative-is-the-best-option-for-web-scraping/</link><guid isPermaLink="false">df5a9203-ee58-440d-8f79-e72a96fcb5fb</guid><category><![CDATA[scrapy alternative]]></category><category><![CDATA[Web Scraping Tool]]></category><category><![CDATA[ParseHub comparison]]></category><category><![CDATA[portia alternative]]></category><category><![CDATA[scrapinghub]]></category><dc:creator><![CDATA[Quentin Simms]]></dc:creator><pubDate>Thu, 11 Aug 2016 21:18:45 GMT</pubDate><content:encoded><![CDATA[<p><a href="https://scrapinghub.com/portia/">Portia 2.0</a>, the newest version of ScrapingHub's visual web scraping tool, is available for beta testing. It already feels like an improvement on the previous version, but I decided to put it head to head with <a href="https://www.parsehub.com/features">ParseHub</a> to see how the two tools compare. <strong>Here is everything you need to know when deciding which web scraping tool better suits your needs</strong>.</p>

<h2 id="howtoscrapethewebwithparsehub">How to scrape the web with ParseHub</h2>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/whoops_stole_Angelinas_logos.png" alt="Open a website, click to select data, download results" class="img-responsive" width="700px"></p>

<p>Choosing data to extract with ParseHub is as easy as clicking on the web page. What is unique about ParseHub, though, is that it can be <strong>instructed to do more than just extract data</strong>. It has a variety of <a href="https://help.parsehub.com/hc/en-us/articles/217730198-Commands-in-the-tool-box-Reference">commands</a> to choose from, making it possible to get data from interactive websites.</p>

<p>Using ParseHub's commands, you can</p>

<ul>
<li><a href="https://help.parsehub.com/hc/en-us/articles/218185307-Get-data-from-behind-a-log-in">sign in to accounts</a></li>
<li>select choices from <a href="https://help.parsehub.com/hc/en-us/articles/217752908-Click">dropdown menus</a>, radio buttons and tabs</li>
<li><a href="https://help.parsehub.com/hc/en-us/articles/218187597-Search-for-one-keyword-using-a-search-box">search with a search bar</a></li>
<li>travel to a new page simply by <a href="https://help.parsehub.com/hc/en-us/articles/217735328-Click-on-the-Next-button-to-scrape-multiple-pages-pagination-">clicking on a "next" button</a>. </li>
<li>get data from infinitely scrolling pages</li>
</ul>

<p>and more. One ParseHub <a href="https://help.parsehub.com/hc/en-us/articles/218181287-Create-your-first-project">project</a> can contain multiple different templates, which lets you <strong>travel between pages with <a href="https://help.parsehub.com/hc/en-us/articles/217735288-Click-on-one-url-to-scrape-a-new-page">different layouts</a></strong> to get all of the data that you need.</p>

<p><em>This is an example ParseHub project. You can see all of the different commands that the user entered, like Select, Hover, and Extract, in the left side bar.</em></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/parsehub_example_billboard.png" alt="scrape billboard 100 with ParseHub" class="img-responsive" width="700px"></p>

<h2 id="howtoscrapethewebwithportia">How to scrape the web with Portia</h2>

<p>Training a sample in Portia is very similar to training a ParseHub template. Like ParseHub, if you click on the first two items on a page, then the rest will be selected for you. <strong>Portia has a much different approach to navigating between web pages</strong>, however.</p>

<p>Unlike ParseHub, you don't tell Portia which pages to travel to. Instead, when you run a spider, <strong>it will continually search the website</strong> that you are on, <a href="http://portia.readthedocs.io/en/latest/examples.html#crawling-paginated-listings">trying to find pages</a> that are structured the same as the sample you have created. This continues until you either tell it to stop, you reach the limit of your ScrapingHub plan, or the software thinks it has checked every page. <strong>This would be useful if you want as much data as possible, without knowing where to find it.</strong></p>

<p>To a ParseHub user who is used to being able to tell their program exactly where to travel, however, this sounds like chaos. And it certainly does lead to unexpected and unwanted data in your results. If you notice that there is a pattern in the URLs of the pages that you want to scrape vs URLs that you don't want to scrape, then <strong>Portia lets you use regular expressions to narrow down its search</strong>. However, big sites like ebay and Amazon do not have predictable or unique URLs, making it impossible to control your navigation this way.</p>

<p><em>This is an example of a Portia sample. The links that are highlighted red are ones that do not match the regular expression that the user has entered, and will therefore not be explored by Portia.</em></p>

<p><img src="http://portia.readthedocs.io/en/2.0-docs/_static/portia-follow-patterns.png" alt="scrape timberland with Portia" class="img-responsive" width="700px"></p>

<p>As explained earlier, Portia spiders can't work together the same way that ParseHub templates can. When they crawl, they get data only from pages that have the exact same layout. Slight variations in layout can be accounted for using <a href="http://portia.readthedocs.io/en/latest/samples.html#multiple-samples">this tutorial</a>), but going between <a href="https://help.parsehub.com/hc/en-us/articles/218186697-Click-on-many-urls-to-scrape-multiple-pages-">search results and more detailed product description pages</a> is not possible.</p>

<p>Here is a comparison between the ParseHub and Portia features:</p>

<table>  
<tr><th>Feature</th><th>ParseHub</th><th>Portia</th></tr><tr>  
</tr><tr><td><i>Environment</i></td>  
<td>Desktop app for Mac, Windows and Linux</td>  
<td>Web based application</td></tr>

<tr><td><i>Selecting elements</i></td>  
<td>Point-and-click, CSS selectors, XPath</td>  
<td>Point-and-click, CSS selectors, XPath</td></tr>

<tr><td><i>Pagination</i></td>  
<td><mark>By clicking on links, entering forms or with URLs</mark></td>  
<td>Exclusively by exploration</td></tr>

<tr><td><i>Scraper logic</i></td>  
<td>Variables, loops, conditionals, function calls (via templates)</td>  
<td>Selecting and extracting only</td></tr>

<tr><td><i>Drop downs, tabs, radio buttons, hovering</i></td>  
<td><mark>Yes</mark></td>  
<td>No</td></tr>

<tr><td><i>Signing in to accounts</i></td>  
<td>Yes</td>  
<td>Yes</td></tr>

<tr><td><i>Entering into search boxes</i></td>  
<td><mark>Yes</mark></td>  
<td>No</td></tr>

<tr><td><i>Javascript</i></td>  
<td>Yes</td>  
<td>Yes, when subscribed to <a href="https://scrapinghub.com/pricing/#splash">Splash</a>

</td></tr><tr><td><i>Debugging</i></td>  
<td>Visual debugger and server snapshots</td>  
<td>Visual debugger and server snapshots</td></tr>

<tr><td><i>Transforming data</i></td>  
<td><a href="https://help.parsehub.com/hc/en-us/articles/217736078-Regular-Expressions">Regex</a>, javascript expressions</td>  
<td><a href="http://portia.readthedocs.io/en/latest/samples.html#partial-annotations">Partial annotations</a></td></tr>

<tr><td><i>Speed</i></td>  
<td>Fast parallel execution</td>  
<td>Fast parallel execution</td></tr>

<tr><td><i>Hosting</i></td>  
<td>Hosted on cloud of hundreds of ParseHub servers</td>  
<td>Hosted on cloud of ScrapingHub servers if subscribed to <a href="https://scrapinghub.com/scrapy-cloud/">Scrapy cloud</a></td></tr>

<tr><td><i>IP Rotation</i></td>  
<td>Included in paid plans</td>  
<td>With Crawlera plan</td></tr>

<tr><td><i>Scheduling runs</i></td>  
<td>With a premium ParseHub account</td>  
<td>With a ScrapyCloud plan</td></tr>

<tr><td><i>Support</i></td>  
<td><mark>Free professional support</mark></td>  
<td><a href="https://support.scrapinghub.com/">Community support</a></td></tr>

<tr><td><i>Data export</i></td>  
<td>CSV, JSON, API</td>  
<td>CSV, JSON, XML, API</td></tr>  
</table>

<h2 id="cost">Cost</h2>

<p>You run your Portia spiders on the same <a href="https://scrapinghub.com/scrapy-cloud/">Scrapy Cloud</a> service that ScrapingHub has offered to <a href="https://www.parsehub.com/blog/parsehub-vs-scrapy-comparison-which-alternative-is-better-for-web-scraping/">Scrapy spiders</a> for years. This lets you run your Portia spiders on the ScrapingHub servers and saves your data online. Buying additional <a href="https://scrapinghub.com/pricing/#scrapy-cloud">Scrapy cloud units</a> makes your crawling faster. Also, the free plan will save your data for only 7 days on the cloud. <strong>If you buy one cloud unit, this will increase to 120 days</strong>.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/Screenshot-2016-07-14-18-47-21.png" alt="scrapinghub cloud unit price: $9 each" class="img-responsive" width="650px"></p>

<p>Many web scraping projects will <strong>not be possible without subscribing to ScrapingHub's other paid services</strong>, however. The pricing of their IP rotation service <a href="https://scrapinghub.com/crawlera/">Crawlera</a> and their JavaScript friendly browser <a href="https://scrapinghub.com/splash/">Splash</a> below, found from the official ScrapingHub <a href="https://scrapinghub.com/pricing/">pricing page</a>. If you are happy to run Portia on your own infrastructure without any of these additions, then you can do so: Portia is an open source software like Scrapy. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/crawlera_pricing_chart.png" alt="crawlera monthly plans" class="img-responsive" width="700px"></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/splash_pricing_chart.png" alt="splash monthly plans" class="img-responsive" width="450px"></p>

<p>Like ScrapingHub, ParseHub offers more speed for more expensive plans. A <a href="https://parsehub.com/pricing">ParseHub subscription</a> allows you to run your projects on the ParseHub servers and save on the cloud, accessible from anywhere, just like a subscription to the Scrapy cloud. Like Splash, ParseHub <strong>handles JavaScript and blocks ads</strong> and, like Crawlera, the premium ParseHub plans have <strong>automatic IP rotation</strong>. You can see a summary of ParseHub's <a href="https://parsehub.com/pricing">pricing plans</a> below.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/parsehub_pricing_portia_comparison.png" alt="ParseHub monthly plans" class="img-responsive" width="700px"></p>

<p>ScrapingHub's incremental plans make it possible for you to <strong>customize your plan to suit your personal needs</strong>. You might need to do some calculating to find out whether ParseHub's plans are a better deal for you, since it will be different for everyone! Make sure to <a href="https://www.parsehub.com/contact">contact ParseHub</a> for a custom solution if you feel like you need a more customized web scraping plan.</p>

<h2 id="exampleproject">Example project</h2>

<p>The clothing website <a href="https://www.parsehub.com/blog/portia-vs-parsehub-comparison-which-alternative-is-the-best-option-for-web-scraping/asos.com">asos.com</a> is having a huge clearance sale, and I want both the sale prices and the regular prices of a variety of items so that I can compare them to my own prices. Let's see how I approached this problem with both web scraping tools!</p>

<h3 id="parsehubexample">ParseHub example</h3>

<p>This is how the <a href="http://www.asos.com/?hrd=1">asos home page</a> looks in the ParseHub desktop app.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/asos_home_parsehub.png" alt="asos home page in ParseHub" class="img-responsive" width="700px"></p>

<p>I started a new project and <a href="https://help.parsehub.com/hc/en-us/articles/217753378-Select">Selected</a> the "view men" and "view women" buttons. I used <a href="https://help.parsehub.com/hc/en-us/articles/217736078-Regular-Expressions">regular expressions</a> to get rid of the word "view" at the start, so that only "women" and "men" got extracted.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/extract_gender_asos.png" alt="extracting the gender from asos clearance" class="img-responsive" width="700px"></p>

<p>To tell ParseHub to travel to both pages, I simply added a <a href="https://help.parsehub.com/hc/en-us/articles/217752908-Click">Click command</a>.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/going_to_asos_categories.png" alt="travelling to asos categories pages" class="img-responsive" width="700px"></p>

<p>ParseHub loaded a page that listed the women's clothing categories. I clicked on the first two to select all 8, extracted the category name and added another Click command to travel to the sales.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/going_to_asos_products.png" alt="travelling to asos sale products" class="img-responsive" width="700px"></p>

<p>ParseHub loaded the first category for me. I decided that I would only need the first page of results, so I clicked on the "SORT BY" drop down menu and selected "What's new".</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/asos_select_whats_new.png" alt="selecting the newest sales on asos" class="img-responsive" width="700px"></p>

<p>I added a Select command and clicked on the names of the first two sale items. The other 34 on the page were automatically selected for me. I added a <a href="https://help.parsehub.com/hc/en-us/articles/218226157-Relative-Select">Relative Select</a> command to select the recommended retail price (RRP) below the name. Clicking on the first one selected all 36.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/selecting_the_rrp_asos.png" alt="using Relative Select to get the RRP" class="img-responsive" width="700px"></p>

<p>I did the same for the sale price below it. Both the RRP and the sale price started with "C$" to represent Canadian dollars, so I used regular expressions again to extract only the number.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/regex_on_asos_parsehub.png" alt="using ParseHub regex to extract the sale price" class="img-responsive" width="700px"></p>

<p>I also decided to extract the image url, just to make product comparison easier. Keep in mind that all of these things will be extracted for each category. For men's clothing, too.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/parsehub_image_asos_sales.png" alt="selecting item images from asos with ParseHub" class="img-responsive" width="700px"></p>

<p>My project was complete and I was ready to get my data. I clicked "Get data" and ran the project once. It returned the information to 577 products, scraped 19 pages and took 3 minutes to complete, on top of the 5-6 minutes that it took me to finish the project. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/parsehub_asos_run_data.png" alt="project run data" class="img-responsive" width="300px"></p>

<p>For someone with less experience with ParseHub, that may have taken a few minutes longer, of course. But the website was very easy to scrape and I ran into no problems during this project.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/asos_results_1_parsehub.png" alt="sample of the results from the asos run run" class="img-responsive" width="700px"></p>

<h3 id="portiaproject">Portia project</h3>

<p>I created a <a href="http://portia.readthedocs.io/en/2.0-docs/projects.html">new project</a> in Portia and started a <a href="http://portia.readthedocs.io/en/2.0-docs/spiders.html">spider</a> on the <a href="http://www.asos.com/?hrd=1">asos home page</a>. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/portia_asos_home.png" alt="asos home page with Portia" class="img-responsive" width="700px"></p>

<p>I had to travel to one of the product pages to train Portia with a sample. I clicked on the "view women" button and then selected the first clothing category out of the eight. I clicked on the <a href="http://portia.readthedocs.io/en/2.0-docs/samples.html">"New Sample" button</a> to begin annotating on this page.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/portia_training_first_sample.png" alt="creating a new sample for portia" class="img-responsive" width="700px"></p>

<p>Clicking on the first two names selected all of the products on the page, just like ParseHub. In fact, Portia was able to do all of the things that ParseHub was during this stage in the project: Also just I used a <a href="http://portia.readthedocs.io/en/2.0-docs/samples.html#extractors">regular expression</a> to strip the euro sign from before each one price, I added <a href="http://portia.readthedocs.io/en/2.0-docs/samples.html#multiple-fields">multiple fields</a> to extract both the name of the product and the URL.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/regular_expression_with_portia_prices.png" alt="creating a new regular expression for prices in portia" class="img-responsive" width="700px"></p>

<p>Then I added the sale price and the picture, with a single click each, and closed the sample.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/portia_more_prices_and_images.png" alt="selecting the rest of the data with Portia" class="img-responsive" width="700px"></p>

<p>Now I just had to tell Portia which pages I wanted it to extract. Recall that I can't give it instructions to click on the buttons that I want it to click on. But, if I can <a href="https://support.scrapinghub.com/topics/1703-improving-crawling-efficiency-with-portia/">find a pattern in the URLs</a>, I can write a regular expression that tells it to scrape only those pages that match the pattern.</p>

<p>So I opened a new tab and took a look at the pages that I wanted Portia to go to. In this case, I did notice a pattern: the URLs of the pages that ParseHub scraped all ended with the pattern <strong>&amp;sort=freshness</strong>. They seemed to be the only pages that ended with that pattern. </p>

<p>So I chose to "Configure url patterns" and told Portia to follow links that followed the pattern <code>.*&amp;sort=freshness</code></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/portia_configure_link_pattern_asos.png" alt="configuring links with a regular expression" class="img-responsive" width="700px"></p>

<p>I toggled on <a href="http://portia.readthedocs.io/en/2.0-docs/examples.html#crawling-paginated-listings">link highlighting</a> to make sure that Portia was able to find the pages I wanted it to go to. It didn't seem like the software was able to follow links from the dropdown menu. I ran the spider regardless, to see if Portia could find a way around it.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/portia_toggle_link_highlight.png" alt="toggling on Portia's link highlighting" class="img-responsive" width="700px"></p>

<p><em>But it didn't</em>. The project completed after Portia couldn't find any links that matched my regular expression, seemingly because it couldn't interact with the dropdown menu. I tried to add extra regular expressions, in an attempt to lead the spider in the direction I wanted it to go, but nothing I tried worked. I got rid of the regular expressions completely and let Portia run wild, on any page that matched my sample. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/portia_run_data.png" alt="Portia run data" class="img-responsive" width="700px"></p>

<p>The spider scraped 10 pages in 6 minutes, 8 of which matched my sample and extracted data. This isn't a bad haul of data, but nowhere near as fast or as effective as the ParseHub run, which scraped 19 pages in less than 3 minutes. <strong>Without buying additional cloud units every month, Portia just doesn't seem fast enough</strong>.</p>

<p>And remember, <strong>I was able to choose exactly which pages I wanted to scrape with ParseHub</strong>. With Portia, I had no way of knowing which 10 pages it would get its data from, because I couldn't control the scrape with any regular expressions. </p>

<h2 id="whatinoticedduringthesampleprojects">What I noticed during the sample projects</h2>

<h3 id="buildspeedandstabilityparsehubdelivers">Build speed and stability: ParseHub delivers</h3>

<p>Compared to Portia, <strong>ParseHub feels much quicker and, most importantly, more stable</strong>. ParseHub let me mouse over anything on the page, and the element highlighted without any lagging. Once the element was clicked on, the rest of the elements that I want were selected immediately. <strong>I never once had to worry whether or not something was broken</strong>.</p>

<p>On the other hand, working with Portia was full of <strong>delays, unpredictability</strong>, and a never-ending stream of the <strong>error messages</strong> like the ones that you see below. The program lags every time you mouse over something to highlight it. To select something, it sometimes takes 2 or 3 clicks, and up to 5 or 10 seconds of nervous waiting to see whether or not something broke, or if the program was just struggling to keep up with the clicks. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/portia_error.png" alt="creating a new regular expression for prices in portia" class="img-responsive" width="700px"></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/portia_many_errors.png" alt="creating a new regular expression for prices in portia" class="img-responsive" width="700px"></p>

<p>I opened my laptop's Activity Monitor to see if it could tell me the reasons for Portia's delays. When building a ParseHub project, the CPU usage hovered between 6% and 9%.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/parsehub_cpu_usage.png" alt="creating a new regular expression for prices in portia" class="img-responsive" width="700px"></p>

<p>When training a Portia sample, the usage stayed much higher, hovering between 15% to 19%.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/portia_cpu_usage.png" alt="creating a new regular expression for prices in portia" class="img-responsive" width="700px"></p>

<p>Here you can see the CPU usage start to drop when I close the sample and go to the ScrapingHub dashboard instead.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/portia_cpu_usage_after.png" alt="creating a new regular expression for prices in portia" class="img-responsive" width="700px"></p>

<p>I suspect that since Portia is browser based, it just requires more resources than the desktop based ParseHub, leading to more lagging and more problems. A few times, I found fields that I had deleted spontaneously reappear, as well as fields I had added spontaneously disappeared. You never knew what Portia was going to do next!</p>

<h3 id="controllednavigationnotpossiblewithportia">Controlled navigation: not possible with Portia</h3>

<p>There may be some websites with URL patterns that are easy to find and predict, but big sites like asos don't work like that. Portia was not able to find the pages that I told ParseHub to go to, seemingly because of two reasons: because there was a page in between my starting page and the pages I wanted to scrape, and because they were behind drop down menus that Portia couldn't interact with.</p>

<p>Plus, to find the patterns in the first place required me to open the pages in new tabs, look back and forth between the long URLs searching for a few characters that looked the same. It was an annoying process, and it yielded no results.</p>

<h3 id="organizationparsehubhastheedge">Organization: ParseHub has the edge</h3>

<p>Because of the uncontrolled extraction of Portia, it makes it <strong>impossible to organize the data as a database</strong>. I extracted the gender and the category of each item of clothing with ParseHub, giving my results a much better structure. You could select all of the women's shoes with simple SQL queries.</p>

<p>It isn't possible to do this with the data I got from Portia, because you had no way of knowing which page your data came from: the spider just happened to stumble across it, without any indication as to what the items are.</p>

<h3 id="modificationsyoucandosomuchmorewithnewparsehubtemplates">Modifications: You can do so much more with new ParseHub templates</h3>

<p>Clicking on the name of each product takes you to a page with additional details. If you want the <strong>product code or the description</strong>, then you can get ParseHub to <a href="https://help.parsehub.com/hc/en-us/articles/218186697-Click-on-many-urls-to-scrape-multiple-pages-">click on each link</a> and start scraping with a new template! I added this new template to my project and got the following resuts:</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/08/parsehub_asos_run_2_data.png" alt="creating a new regular expression for prices in portia" class="img-responsive" width="300px"></p>

<p>It took longer to get this data because ParseHub had to travel to almost 600 pages. However, because of parallel execution, the job was finished in just under 27 minutes. </p>

<h2 id="feedbackletmeknowwhatyouthink">Feedback: Let me know what you think!</h2>

<p>I want this comparison to be as fair as possible. However, I can't help if my bias for ParseHub may have lead to some unfair criticism of Portia. <strong>If you are an experienced Portia user and have noticed something wrong with this review, please let me know in the comments or in a personal email to quentin[at]parsehub[dot]com</strong>. This includes: suggesting changes to my spider to make it serve my use case better, features that I missed that give it an edge over ParseHub, Portia tips and tricks, etc. I will be sure to review everyone's feedback and make changes to make this review fair and objective. </p>]]></content:encoded></item><item><title><![CDATA[How ParseHub is helping the freedom of information in Brazil]]></title><description><![CDATA[Web scraping has changed the way the non-profit Abraji monitors lawsuits against the media. See how ParseHub is bringing this information to the public!]]></description><link>https://www.parsehub.com/blog/how-parsehub-is-helping-the-freedom-of-information-in-brazil-2/</link><guid isPermaLink="false">1f4e8284-27c2-470e-b0fd-22bb68bf2ced</guid><category><![CDATA[Web Scraping]]></category><category><![CDATA[nonprofit organization]]></category><category><![CDATA[use case]]></category><category><![CDATA[investigative journalism]]></category><dc:creator><![CDATA[Tiago Mali]]></dc:creator><pubDate>Thu, 04 Aug 2016 17:53:00 GMT</pubDate><media:content url="/content/images/2016/07/abraji_increase_in_lawsuits.png" medium="image"/><content:encoded><![CDATA[<img src="/content/images/2016/07/abraji_increase_in_lawsuits.png" alt="How ParseHub is helping the freedom of information in Brazil"><p><img src="http://congresso.abraji.org.br/images/logo2013.jpg" alt="How ParseHub is helping the freedom of information in Brazil" class="img-responsive" width="600px"></p>

<p>There are countless Brazilian journalists and media companies who are facing lawsuits because of what they have published online. Small media outlets, without the funds to fight back in court, are left with only one option: to remove their content from the web. The <a href="http://www.ctrlx.org.br/noticia/acoes-judiciais-contra-a-imprensa-devem-aumentar-em-2016-afirmam-especialistas">number of lawsuits is increasing</a>, meaning that the threat to silence information in Brazil is increasing. </p>

<p><em>In the visualization below, you can see an increase in the number of lawsuits that were filed in the months leading to the 2014 election. See the <a href="http://www.ctrlx.org.br/#/infografico/geral/estado/shData:1%2F2016,2%2F2016,3%2F2016,4%2F2016,5%2F2016,6%2F2016,7%2F2016,8%2F2016,9%2F2016,10%2F2016,11%2F2016,12%2F2016">Ctrl+X website</a> for more interactive visualizations.</em>
 <img src="https://www.parsehub.com/blog/content/images/2016/07/abraji_increase_in_lawsuits.png" alt="How ParseHub is helping the freedom of information in Brazil" class="img-responsive" width="700px"></p>

<p>We, at the <a href="http://abraji.org.br/">Brazilian Association for Investigative Journalism</a> (Abraji), are deeply concerned with the effects of these content removal requests. In our country, the main purpose of many lawsuits is to intimidate and control. Aware that one of the biggest source of lawsuits against online publications are the politicians who are campaigning for elections, in 2014 we launched a project with the intention of tracking and investigating these requests. The project was then called “Eleições Transparentes” (Transparency in Elections). During the 2014 electoral race, the project identified the politicians and the parties that most frequently filed suits against journalists and media outlets.</p>

<p>Eleições Transparentes found that there is no shortage of politicians <a href="http://www.ctrlx.org.br/noticia/eleicoes-de-2014-tiveram-quase-200-processos-contra-divulgacao-de-informacoes">requesting the censorship</a> of journalists and media outlets alike. We have also started seeing the first instances where judges have accepted these requests. The most popular case was in 2009, when a court ordered that one of the biggest newspapers in Brazil, O Estado de S. Paulo, to <a href="http://politica.estadao.com.br/noticias/geral,justica-censura-estado-e-proibe-informacoes-sobre-sarney,411711">cease publishing</a> news about a political scandal involving Fernando Sarney, the son of a former president.</p>

<p><em>Visualization of media companies by the number of content removal lawsuits they have received.</em></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/abraji_media_companies.png" alt="How ParseHub is helping the freedom of information in Brazil" class="img-responsive" width="700px"></p>

<p><em>Visualization of parties by the number of lawsuits they have filed.</em></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/abraji_parties_suing_most.png" alt="How ParseHub is helping the freedom of information in Brazil" class="img-responsive" width="700px"></p>

<p>In Abraji we believe that the best way to fight against this problem, which undermines the freedom of expression in our country, is to bring publicity to these lawsuits. Our goal is to make it easy to find information on content removal requests, facilitating research and allowing the public to keep themselves informed. This information includes the reason why the request was filed, who is behind it, and to what they are asking for. We gather this information directly from the law departments of the biggest media outlets in Brazil, and also by leaving an open channel for every independent journalist, blogger or citizen involved in one of these lawsuits. This encourages everyone to send us the details of their case and bring awareness to it.</p>

<p>We realized, though, that direct contact with journalists is not the only way to monitor Brazilian censorship. We would need more information to be more precise and more efficient. Since 2015 we have been updating the project, <strong>now called <a href="http://www.ctrlx.org.br/#/infografico/geral/estado/shData:1%2F2016,2%2F2016,3%2F2016,4%2F2016,5%2F2016,6%2F2016,7%2F2016,8%2F2016,9%2F2016,10%2F2016,11%2F2016,12%2F2016">Ctrl+X</a></strong>, with new approaches to the problem. We discovered that it was very difficult to search through the court websites, and it is not possible to search through all of the different state court websites at once.</p>

<p><strong>This is where <a href="https://parsehub.com/web-scraping-for-data-science">ParseHub's</a> help came in.</strong></p>

<p>In preparation for the 2016 elections, we started to make regular queries to track the new lawsuits involving candidates. This proved to be too many queries for a team to enter manually. For each media company that we wanted to search for as a participant in the lawsuit (and there are many of them), its name had to be searched in 28 different electoral courts. For each court results, there are multiple ways (more than 50 different ways in some cases) that this media company's name could appear. We had to dig into each one of these options to find and read the lawsuits related to it. A very slow process that takes an enormous amount of time to do manually.</p>

<p>We created a <a href="https://help.parsehub.com/hc/en-us/articles/218181287-Create-your-first-project">ParseHub project</a> that searches for all of these different terms in all the electoral courts at once. There are a lot of lawsuits that are returned in these queries, so we also created a filter that extracts only the lawsuits dated in 2016 (the previous ones we already did manually). As a result, we get a spreadsheet with the information to every lawsuit related to the project. Each time we run our ParseHub project, it searches through more than 20 thousand web pages and returns only the results that are relevant to our cause.</p>

<p><img src="https://www.parsehub.com/static/images/howitworks2.png" alt="How ParseHub is helping the freedom of information in Brazil" class="img-responsive" width="700px"></p>

<p>This has saved us tons of time, especially because we are making weekly scrapes. Now we can compare different scrapes to see only the new lawsuits of the last week. We can not thank ParseHub’s team enough for providing us with such a powerful tool and making it easier to monitor what the Brazilian politicians are trying to hide in the internet. <strong>So far, we managed to gather the information of more than <a href="http://www.ctrlx.org.br/oficios">1.7 k lawsuits</a></strong>. With ParseHub’s help we are keeping an open eye on censorship, and have improved the precision of our project enormously.</p>]]></content:encoded></item><item><title><![CDATA[Pull data from calendar to make a festival schedule]]></title><description><![CDATA[An out-of-the-ordinary use of a web scraping tool: making printable festival schedules when none could be found online. ]]></description><link>https://www.parsehub.com/blog/making-my-own-music-festival-schedule-by-scraping-the-web-2/</link><guid isPermaLink="false">f5e549c9-bbd5-44e3-a4b1-d7c32db2535e</guid><category><![CDATA[Web Scraping]]></category><category><![CDATA[python]]></category><category><![CDATA[music festival]]></category><category><![CDATA[schedule]]></category><dc:creator><![CDATA[Quentin Simms]]></dc:creator><pubDate>Mon, 18 Jul 2016 16:56:46 GMT</pubDate><content:encoded><![CDATA[<p>Here's a quick little story about an unconventional use for web scraping. When inspiration hits you, though, sometimes you've got to run with it, right?</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/e/e8/2012-08-03_Osheaga_Festival_Parc_Jean-Drapeau_Montreal.jpg" alt="Osheaga music festival crowd" class="img-responsive" width="700px"></p>

<p>It's seems to be a tradition among Torontonians to roast in the sun at an <a href="http://indie88.com/2016-guide-to-canadian-music-festivals/">outdoor music festivals</a> at some point during the summer - we're a little obsessed with them. You can't blame us though, there's no better way to celebrate the end of the 5 or 6 months when the city really does become the "Great White North". I wasn't planning on going to one this summer, but friends of mine managed to convince me to impulse-buy a ticket to <a href="http://wayhome.com/">WayHome Music Festival</a> last weekend. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/wayhome_schedule.png" alt="2016 WayHome schedule" class="img-responsive" width="500px"></p>

<h2 id="theproblem">The Problem</h2>

<p>The festival is only a few days away from today, and I was planning who I wanted to see. Understand that I am "that guy" in the group - the one that plans to run from stage to stage to see as much music as I possibly can, so it has to be planned out all ahead of time.</p>

<p>Now, I've got to hand it to WayHome, the <a href="http://lineup.wayhome.com/events/2016/07/22/">online schedule</a> looks awesome. But the problem is, <strong>I can't find a printable schedule anywhere</strong>. It seems to be the one thing they forgot to put online!</p>

<h2 id="thesolution">The Solution</h2>

<p>Probably because of how much I'd been using <a href="https://www.parsehub.com/features">ParseHub</a> in the last few days, I began wondering if it was versatile enough to dates and times from the schedule, which seemed to have a ton of interactive elements. And that's when the crazy idea hit me: <strong>make my own printable schedules using the data online</strong>. I had no idea if I was going to be able to do it, but I wanted a break from work so I gave it a shot.</p>

<p>To my pleasant surprise, ParseHub was able to handle the interactive schedule really well! In fact, the project worked exactly how I wanted to, first try. I had the data in less than 5 minutes. You can take a look at my project below.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/wayhome_parsehub_projects.png" alt="ParseHub project to scrape WayHome schedule" class="img-responsive" width="700px"></p>

<p>Now I just had to find a way to visualize it! I am by no means an experienced programmer, especially when it comes to creating graphics. But I did learn how to make bar graphs using Python for a few previous projects... and a schedule is sort of like a bar graph? You might disagree, but hey I told you this story was going to be be a little unconventional. </p>

<p><img src="http://www.ybrikman.com/assets/img/blog/github-pages/shopping-cart-grill.jpg" alt="If it looks stupid but works" class="img-responsive" width="500px"></p>

<p>You can find the result in the three links below. Not too bad for an hours work, huh? They will certainly make group planning easier. Notice that AM and PM is flipped, as in 1 pm at the bottom of the schedule is "1" and 2 am at the top of the schedule is "14". </p>

<p><a href="https://www.parsehub.com/blog/content/images/2016/07/Friday_Schedule.png">Friday Schedule</a></p>

<p><a href="https://www.parsehub.com/blog/content/images/2016/07/Saturday_Schedule.png">Saturday Schedule</a></p>

<p><a href="https://www.parsehub.com/blog/content/images/2016/07/Sunday_Schedule.png">Sunday Schedule</a></p>

<p>If you are going to the festival yourself, feel free to download the schedules, give them to your friends, etc. Like I mentioned earlier, I used Python to create these schedules. Check out <a href="https://help.parsehub.com/hc/en-us/articles/219619888-Analyzing-JSON-With-Python">this tutorial</a> on using <a href="https://www.parsehub.com/">ParseHub</a> with <a href="http://jupyter.org/">Jupyter Notebook</a> to get familiar. Here is every line of code that I used, separated into cells:</p>

<pre><code>[1]

import pandas as pd  
import pylab as pl  
import numpy as np  
import json  
import re  
</code></pre>

<pre><code>[2]

# load ParseHub JSON
schedule = pd.read_json('Desktop/WAYHOME SCHEDULE', orient='columns')

# Expand into the appropriate columns
schedule = pd.read_json(schedule['artists'].to_json(), orient='index')`

# Add three new empty columns that will be filled later
times = pd.DataFrame(None, columns = ['start', 'end'], index = schedule.index)  
schedule = schedule.join(times)`

# Sort the columns by index in case they got jumbled up for some reason
schedule = schedule.sort_index()  
</code></pre>

<pre><code>[3]

# This loop will fill the "start" and "end" columns for each act.
# Regular expressions will match with either 0, 15, 20, 30, or 45 minutes past the hour
# This will convert to a decimal of the hour (0, 0.25, 0.333, 0.5, 0.75) making plotting easier

for i in schedule.index:

    # this will fill the "start" column with whatever time comes before the " - "
    schedule.loc[i, 'start'] = ( re.search(r'(^[^ -]*)', schedule.loc[i, 'time']) ).group(1)

    # the condition for this if statement will be met if there is a '15' after the colon in the start time
    if ( re.search(r'([^:]*$)', schedule.loc[i, 'start']) ).group(1) == '15' :
        # this will replace the string with a decimal representation of the hour
        schedule.loc[i, 'start'] = int( re.search(r'(^[^:]*)', schedule.loc[i, 'start'] ).group(1) ) + 0.25
    elif ( re.search(r'([^:]*$)', schedule.loc[i, 'start']) ).group(1) == '30' :
        schedule.loc[i, 'start'] = int( re.search(r'(^[^:]*)', schedule.loc[i, 'start'] ).group(1) ) + 0.5
    elif ( re.search(r'([^:]*$)', schedule.loc[i, 'start']) ).group(1) == '45' :
        schedule.loc[i, 'start'] = int( re.search(r'(^[^:]*)', schedule.loc[i, 'start'] ).group(1) ) + 0.75
    elif ( re.search(r'([^:]*$)', schedule.loc[i, 'start']) ).group(1) == '00' :
        schedule.loc[i, 'start'] = int( re.search(r'(^[^:]*)', schedule.loc[i, 'start'] ).group(1) ) + 0

    #this will fill the "end" column with whatever comes after the " - "
    schedule.loc[i, 'end'] = ( re.search(r'([^- ]*$)', schedule.loc[i, 'time']) ).group(1)
    if ( re.search(r'([^:]*$)', schedule.loc[i, 'end']) ).group(1) == '15' :
        schedule.loc[i, 'end'] = int( re.search(r'(^[^:]*)', schedule.loc[i, 'end'] ).group(1) ) + 0.25
    elif ( re.search(r'([^:]*$)', schedule.loc[i, 'end']) ).group(1) == '30' :
        schedule.loc[i, 'end'] = int( re.search(r'(^[^:]*)', schedule.loc[i, 'end'] ).group(1) ) + 0.5
    elif ( re.search(r'([^:]*$)', schedule.loc[i, 'end']) ).group(1) == '45' :
        schedule.loc[i, 'end'] = int( re.search(r'(^[^:]*)', schedule.loc[i, 'end'] ).group(1) ) + 0.75
    elif ( re.search(r'([^:]*$)', schedule.loc[i, 'end']) ).group(1) == '00' :
        schedule.loc[i, 'end'] = int( re.search(r'(^[^:]*)', schedule.loc[i, 'end'] ).group(1) ) + 0
    elif ( re.search(r'([^:]*$)', schedule.loc[i, 'end']) ).group(1) == '20' :
        schedule.loc[i, 'end'] = int( re.search(r'(^[^:]*)', schedule.loc[i, 'end'] ).group(1) ) + 0.333
</code></pre>

<pre><code>[4]

dates = ["Fri, Jul 22", "Sat, Jul 23", "Sun, Jul 24"]  
stages = ["WayHome", "WayBright", "WayBold", "WayAway"]  
# hard coded because apparently matlplotlib can't automatically wrap text in a text box!
schedule.loc[schedule['name'] == 'Nathaniel Rateliff &amp; the Night Sweats', 'name'] = 'Nathaniel Rateliff &amp; the Night\nSweats'  
</code></pre>

<pre><code>[5]

# this function will plot each stage's schedule

def barplot(stage) :

    count = 0

    # this will step through the stage's entire index
    for i in stage.index :

        count += 1

        # start and end times
        start = stage.loc[i, 'start']
        end = stage.loc[i, 'end']

        # these two conditions will handle the problem that there is no AM / PM distinction in time

        # if the show starts before midnight but ends after midnight, adds 12 to the end time
        if start &gt; end:
            end = end + 12

        # The latest shows start at 1 am, and the earliest shows start at 1 pm. 
        # To distinguish them, the "count" variable will only equal 1 for the 1 pm shows, 
        # because of the order of the data extraction
        # Therefore, if the start time is less than 2 and count is not 1, the time is 1 am = 13
        if start &lt; 2 and count != 1 :
            start = start + 12
            end = end + 12

        # height of the bar that will be plotted
        height = end - start
        # text y location will be half way between start and finish
        text_y = height/2 + start

        # plot the bars
        pl.bar(xlocation, height, bottom=start, color=color, edgecolor='black', linewidth = 3.0)
        # artist name text
        pl.text(xlocation+0.4, text_y, stage.loc[i, 'name'], ha='center')
        # show time text will be located just below the artist name
        pl.text(xlocation+0.4, text_y - 0.3, stage.loc[i, 'time'], ha='center')
</code></pre>

<pre><code>[6]

%matplotlib inline

# will plot each of the days in a separate figure
for date in dates:

    pl.figure(figsize=[15,22])
    pl.xticks(np.arange(0,4) + 0.5, stages)

    # first plot the horizontal lines as thin black boxes

    height = 0.01
    color = 'black'
    width = 4

    for j in range(2,14):
        pl.bar(0, height, width, bottom=j, color=color, edgecolor='black')

    color = "beige"
    width = 0.8

    # Next split the day's schedule into each stage and plot them in the correct x location

    day_schedule = schedule[schedule["date"] == date]

    wayhome = day_schedule[day_schedule['stage'] == 'WayHome']
    xlocation = 0.1
    barplot(wayhome)

    waybright = day_schedule[day_schedule['stage'] == 'WayBright']
    xlocation = 1.1
    barplot(waybright)

    waybold = day_schedule[day_schedule['stage'] == 'WayBold']
    xlocation = 2.1
    barplot(waybold)

    wayaway = day_schedule[day_schedule['stage'] == 'WayAway']
    xlocation = 3.1
    barplot(wayaway)

    pl.title(date, fontsize=24)
    pl.ylim([1,14])
    pl.show()
</code></pre>]]></content:encoded></item><item><title><![CDATA[ParseHub vs. Scrapy Comparison – which alternative is better for web scraping?]]></title><description><![CDATA[<p><img src="https://www.parsehub.com/blog/content/images/2016/07/Screenshot-2016-07-14-18-53-42.png" alt=""></p>

<p><a href="http://scrapy.org/">Scrapy</a> is probably the most popular open source framework for web scraping. It's been around since at least 2008, which is when I first used it. It started out as an open-source release of a python framework built for scraping a large number for a commercial enterprise. The framework turned</p>]]></description><link>https://www.parsehub.com/blog/parsehub-vs-scrapy-comparison-which-alternative-is-better-for-web-scraping/</link><guid isPermaLink="false">48b912dc-2683-4e37-a8f5-6c87276d7b40</guid><dc:creator><![CDATA[Greg Dingle]]></dc:creator><pubDate>Fri, 15 Jul 2016 23:01:15 GMT</pubDate><content:encoded><![CDATA[<p><img src="https://www.parsehub.com/blog/content/images/2016/07/Screenshot-2016-07-14-18-53-42.png" alt=""></p>

<p><a href="http://scrapy.org/">Scrapy</a> is probably the most popular open source framework for web scraping. It's been around since at least 2008, which is when I first used it. It started out as an open-source release of a python framework built for scraping a large number for a commercial enterprise. The framework turned out to be so successful on its own that the creators of it formed a company around it––<a href="http://scrapinghub.com/">scrapinghub.com</a>.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/Screenshot-2016-07-14-18-56-04.png" alt=""></p>

<p>In this article, I will first compare the visual web scraping tool  ParseHub to Scrapy as an <strong>open-source</strong> python project, and then I will compare ParseHub to the ScrapingHub <strong>paid service</strong> which runs Scrapy spiders for a fee.</p>

<h1 id="parsehubandscrapy">ParseHub and Scrapy</h1>

<p>Comparing ParseHub to Scrapy is somewhat of an <strong>apples-to-oranges</strong> comparison, because one is a <strong>UI tool</strong> and the other is a <strong>programming library</strong>. A more apples-to-apples comparison would be to the associated open-source project <a href="https://github.com/scrapinghub/portia">Portia</a>. But since Scrapy is so established, and Portia is relatively new, I will confine this article to the first comparison, and leave Portia for another day and another blog post.</p>

<table>  
<tr><th>Feature</th><th>ParseHub</th><th>Scrapy</th></tr><tr>  
</tr><tr><td><i>Authoring environment</i></td>  
<td><mark>Desktop</mark> app (Mac, Windows and Linux)</td>  
<td><mark>Python</mark> plus scrapy command line tool</td></tr>

<tr><td><i>Scraper logic</i></td>  
<td>Variables, loops, conditionals, function calls (via templates)</td>  
<td>Variables, loops, conditionals, function calls (arbitrary python)</td></tr>

<tr><td><i>Javascript, Ajax and dynamic content</i></td>  
<td><mark>Yes</mark></td>  
<td>With <mark>external</mark> libraries</td></tr>

<tr><td><i>Pop-ups, infinite scroll, hover content</i></td>  
<td><mark>Yes</mark></td>  
<td>With <mark>external</mark> libraries</td></tr>

<tr><td><i>Debugging</i></td>  
<td>Visual debugger</td>  
<td>Python logs</td></tr>

<tr><td><i>Knowledge of HTML and HTTP</i></td>  
<td>None required</td>  
<td>Required</td></tr>

<tr><td><i>Selecting elements</i></td>  
<td><mark>Point-and-click</mark>, CSS seletors, XPath</td>  
<td>CSS seletors, XPath</td></tr>

<tr><td><i>Transforming data</i></td>  
<td>Regex, javascript expressions</td>  
<td>Regex, arbitrary python</td></tr>

<tr><td><i>Speed</i></td>  
<td>Fast parallel execution</td>  
<td>Fast parallel execution</td></tr>

<tr><td><i>Hosting</i></td>  
<td>Hosted on cloud of hundreds of ParseHub servers</td>  
<td>Hosted on your local machine or your own servers</td></tr>

<tr><td><i>IP Rotation</i></td>  
<td><mark>Included</mark> in paid plans</td>  
<td>Must pay <mark>external</mark> service</td></tr>

<tr><td><i>Sites (AKA spiders, scrapers, projects)</i></td>  
<td>Free plan: 5, $99/month: 20, $499/month: 120</td>  
<td>Limited by your infrastructure</td></tr>

<tr><td><i>Support</i></td>  
<td>Free professional support</td>  
<td>Community support</td></tr>

<tr><td><i>Data export</i></td>  
<td>CSV, JSON, API</td>  
<td>CSV, JSON, API</td></tr>

<tr><td><i>Run-time configuration</i></td>  
<td>Passed in as a JSON object</td>  
<td>Passed in command line, arbitrary python</td></tr>  
</table>

<h2 id="parsehubandscrapyconculsion">ParseHub and Scrapy: Conculsion</h2>

<p><mark>ParseHub offers most of the web scraping power and scale of Scrapy in a <em>much</em> easier-to-use package.</mark> Because we're actually big fans of Scrapy, we still recommend it for a few situations:</p>

<ul>
<li>Tight integration with existing python codebase and infrastructure</li>
<li>Crawling hundreds of websites and grabbing all HTML or just some keywords</li>
</ul>

<p>We are working on solving the second use case with ParseHub right now. Stay tuned!</p>

<h1 id="parsehubandscrapinghub">ParseHub and Scrapinghub</h1>

<p>Scrapinghub is a paid service for running web scrapers (AKA spiders or projects) created with the open-source python framework Scrapy. It is <strong>equivalent</strong> to ParseHub's "run on server" and "run on a schedule" service which is integrated into the ParseHub desktop app.</p>

<p>At first glance, the main difference between the two services appears to be their <strong>pricing</strong>. ParseHub packages  capabilities into conventional <strong>software-as-a-service (SaaS)</strong> plans Free, Standard ($99) and Professional ($499). Scrapinghub prices its service in $9 "scrapy cloud units", similar to <strong>infrastructure-as-a-service (IaaS)</strong> such as Amazon EC2.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/Screenshot-2016-07-14-18-47-21.png" alt=""></p>

<p>But it is easy to see that both services offer a <strong>generous free plan</strong> that grants multiple projects and hundreds or more pages. And both ParseHub and Scrapinghub offer <strong>more speed for more money</strong>. ParseHub clearly defines how many pages a minute it will provide for each plan. Scrapinghub offers additional "concurrent crawls" for $9 each. It would require some benchmarking to estimate how much faster each crawl makes Scrapy in terms of pages per minute.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/Screenshot-2016-07-14-18-49-10.png" alt=""></p>

<p>ParseHub and Scrapinghub both offer <strong>IP rotation</strong>, but Scrapinghub sells it in a separate service, Crawlera, starting at $25 a month and up to $500 or more.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/Screenshot-2016-07-14-15-14-27.png" alt=""></p>

<h2 id="conclusionparsehubandscrapinghub">Conclusion: ParseHub and Scrapinghub</h2>

<p>Like the earlier comparison, ParseHub vs Scrapinghub is somewhat of an apples-to-oranges comparison. ParseHub is designed to work at a <strong>higher level</strong> in which most of the features of Scrapinghub are <strong>bundled together</strong>. <mark>Scrapinghub is a good choice if you are already convinced that Scrapy is for you. If you are just starting out, we encourage you to try ParseHub which will get you up and running much faster and for similar pricing.</mark></p>

<p>As a final note, Scrapinghub's monitoring dashboard is really nice. Kudos.</p>]]></content:encoded></item><item><title><![CDATA[How to get the locations of retail stores with web scraping]]></title><description><![CDATA[Don't spend hours of copying and pasting the addresses, discover the power of web scraping! Learn to automate this task - without writing any code!]]></description><link>https://www.parsehub.com/blog/how-to-get-the-locations-of-retail-stores-with-web-scraping/</link><guid isPermaLink="false">54a50b56-424f-4e2a-ab05-fc0b9672442d</guid><category><![CDATA[How To Guides]]></category><category><![CDATA[Web Scraping]]></category><category><![CDATA[eCommerce]]></category><category><![CDATA[Web Scraping Tool]]></category><category><![CDATA[store locator]]></category><category><![CDATA[location data]]></category><category><![CDATA[Costco]]></category><category><![CDATA[map data]]></category><category><![CDATA[extract data]]></category><category><![CDATA[Business Hacks]]></category><dc:creator><![CDATA[Quentin Simms]]></dc:creator><pubDate>Wed, 13 Jul 2016 16:22:52 GMT</pubDate><media:content url="/content/images/2016/07/starbucks_locations.png" medium="image"/><content:encoded><![CDATA[<img src="/content/images/2016/07/starbucks_locations.png" alt="How to get the locations of retail stores with web scraping"><p><img src="https://www.parsehub.com/blog/content/images/2016/07/starbucks_locations.png" alt="How to get the locations of retail stores with web scraping" class="img-responsive" width="700px"></p>

<p>Most retail stores have a <strong>store locator</strong> map on their website. Customers use it to find their locations with just a quick search, but it is also used by distributors, wholesalers and other retailers to get information on their <a href="https://www.parsehub.com/blog/get-10-000-quality-sales-leads-in-15-minutes/"><strong>sales leads</strong></a> or their <strong>competitors</strong>.</p>

<p>If you have found yourself spending hours of your valuable time copying and pasting the addresses, phone numbers and store hours from these maps into your own database, <strong>then it is about time you discovered the power of web scraping.</strong> In this tutorial you will learn how to create a ParseHub data extraction project to automate this time consuming task - without writing a single line of code.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/whoops_stole_Angelinas_logos.png" alt="How to get the locations of retail stores with web scraping" class="img-responsive" width="700px"></p>

<h3 id="implementwebscrapingintoyourbusinessto">Implement web scraping into your business to:</h3>

<ul>
<li>get your competitors locations to find out where to expand your business</li>
<li>get the locations of businesses that you can distribute to</li>
<li><p>get contact info for all of the important businesses near yours</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/automate_all_the_things.jpg" alt="How to get the locations of retail stores with web scraping" class="img-responsive" width="500px"></p></li>
</ul>

<h2 id="step1startyourwebscrapingproject">Step 1: Start your web scraping project</h2>

<ol>
<li>ParseHub is a visual data extraction tool, meaning you get the data you want just by clicking on it, instead of learning to type in computer code. Download the <a href="https://www.parsehub.com/quickstart">desktop app</a> to get started!  </li>
<li>Open the ParseHub application and travel to the retail store website of your choice. In this example, we will use <a href="http://www.costco.com/">http://www.costco.com/</a>  </li>
<li><p>Once you have traveled to the website, click on "New Project" and "Start project on this URL" to <a href="https://help.parsehub.com/hc/en-us/articles/218181287-Create-your-first-project">create</a> your project.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/costco_home_page.png" alt="How to get the locations of retail stores with web scraping" class="img-responsive" width="700px"></p></li>
</ol>

<h2 id="step2searchingforyourlocation">Step 2: Searching for your location</h2>

<ol>
<li>Now that you have created your web scraping project, you are ready to add the <a href="https://help.parsehub.com/hc/en-us/articles/217753218-What-is-a-command-">commands</a> into your first template. Some templates are used to get data from a page, but this one will be used to <a href="https://help.parsehub.com/hc/en-us/articles/218187597-Search-for-one-keyword-using-a-search-box">search</a> for that data. Click on the "Find a Warehouse" button to select it with the <a href="https://help.parsehub.com/hc/en-us/articles/217753378">Select</a> command.  </li>
<li>Rename your selection from <strong>selection1</strong> to <strong>warehouse_button</strong> by clicking on the command.  </li>
<li>If you were using a regular browser to search, you would hover over this button without clicking to open a popup window. Luckily, you can command ParseHub to hover, the exact same way! You will see a <a href="https://help.parsehub.com/hc/en-us/articles/220053907-Where-is-the-Plus-Button-">"plus" button</a> to the right of the command "Select and Extract <strong>warehouse_button</strong>". Click on it to open the command menu.  </li>
<li>Click on "Advanced" to open the rest of the commands.  </li>
<li><p>Choose the <a href="https://help.parsehub.com/hc/en-us/articles/218226147-Hover">Hover</a> command from the list to add it to your template. The pop up window will open up.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/add_hover_command_costco.png" alt="How to get the locations of retail stores with web scraping" class="img-responsive" width="700px"></p></li>
<li>Add another Select command by clicking on the "plus" button next to the command "Select <strong>page</strong>".  </li>
<li>Click on the "City, State or Zip" search bar and an <a href="https://help.parsehub.com/hc/en-us/articles/217753298-Input">Input</a> command will be added automatically.  </li>
<li>Type in your location into the input box. We will use <strong>San Francisco</strong>.  </li>
<li>Rename your selection <strong>search_bar</strong>.  </li>
<li><p>When you run your project, you will want to make sure that the pop up window appears when ParseHub hovers over it. In order to do this, click on the "Select <strong>search_bar</strong>" command, and check the box that says "Wait up to 60 seconds for elements to appear", and change 60 to 2.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/costco_search_bar_senter.png    " alt="How to get the locations of retail stores with web scraping" class="img-responsive" width="700px"></p></li>
<li>Click on the "plus" button again to add another Select command.  </li>
<li>Click on the "Find a Warehouse" button to select it. Name it <strong>search_button</strong>.  </li>
<li>Click on the "plus" button beside "Select and Extract <strong>search_button</strong>" command to add a Click command from the menu.  </li>
<li><p>Clicking on this button will take you to a new page. Therefore, we want to <strong>Create a New Template</strong>. Call it <strong>search_results</strong>.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/costco_click_search.png    " alt="How to get the locations of retail stores with web scraping" class="img-responsive" width="700px"></p></li>
</ol>

<h2 id="step3extractdatafor10storelocations">Step 3: Extract data for 10 store locations</h2>

<ol>
<li>You will be taken to a map with the 10 closest locations. Click on the first two in the list under the map. This will select and extract all 10 of them.  </li>
<li>Rename the selection <strong>locations</strong>. Click on the check box next to "Wait up to 2 seconds for elements to appear" since the website may take a while to search.  </li>
<li>Click the plus button next to "Begin new entry in <strong>locations</strong>.  </li>
<li>Add a <a href="https://help.parsehub.com/hc/en-us/articles/218226157-Relative-Select">Relative Select</a> command from the command menu.  </li>
<li>Click on one of the names and then click on the address below it. This will select and extract all of the addresses.  </li>
<li><p>Rename the selection <strong>address</strong>.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/relative_select_costco.png    " alt="How to get the locations of retail stores with web scraping" class="img-responsive" width="700px"></p></li>
<li>Add another Relative Select command by clicking the "plus" button next to the "Begin new entry in <strong>locations</strong>" command. To select all of the the city and zip codes, click on the name of one location and then on the city and zip code below it.  </li>
<li><p>Add another Relative Select and select the telephone numbers the same way.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/costco_relative_selects.png    " alt="How to get the locations of retail stores with web scraping" class="img-responsive" width="700px"></p></li>
</ol>

<h2 id="step4scrapeadditionalstoredetailswithpagenavigation">Step 4: Scrape additional store details with page navigation</h2>

<p>Clicking on the name of one of the locations will bring you to that store's additional details page, where you will find the store hours and a list of departments that it has. </p>

<ol>
<li>Click on the "plus" beside the "Begin new entry in <strong>locations</strong>" command, open the Advanced menu and add a Click command. By nesting this command in the <a href="https://help.parsehub.com/hc/en-us/articles/217753318-Begin-New-Entry">Begin New Entry</a> command, you are telling ParseHub to click on each one of the locations!  </li>
<li><p>You will travel to a new page with each click, so Create New Template and call it <strong>details</strong>.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/costco_create_details_template.png" alt="How to get the locations of retail stores with web scraping" class="img-responsive" width="700px"></p></li>
<li><p>You will travel to the first details page in the list. I will first select the store hours, which are split up into three different lines. I don't want them to each get their own row in the CSV, so I will select all of them at once by <a href="https://help.parsehub.com/hc/en-us/articles/220331888-Keyboard-Shortcuts">zooming out</a> by pressing "ctrl" (or "command") and "1" on my keyboard.  </p></li>
<li><p>To select all of the department names, I click on one from each of the columns. This highlights them all, and selects and extracts them into separate rows in the CSV.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/costco_hours_departments.png" alt="How to get the locations of retail stores with web scraping" class="img-responsive" width="700px"></p></li>
</ol>

<h2 id="step5getdata">Step 5: Get data</h2>

<p>So far I have instructed ParseHub to extract the address, phone number store hours and departments of each location near me. That's all the information that I need for my database, so it is time to run my project and extract my data!</p>

<p>There are a few ways to do this. You can <a href="https://help.parsehub.com/hc/en-us/articles/217729938-Run-your-project">run the project once</a> and download your data from the app, you can <a href="https://help.parsehub.com/hc/en-us/articles/218181547-Schedule-Project">schedule the project</a> to run and download the data from an email, or you can control the projectusing  ParseHub's <a href="https://www.parsehub.com/docs/ref/api/v2/#run-a-project">API options</a> with your own script. Here I will show you how to run the project once.</p>

<ol>
<li>Click on the big green "Get Data" button in the ParseHub side bar.  </li>
<li>Click "Run" and "Save and Run" and wait for the extraction to complete.  </li>
<li><p>Download the CSV and the JSON. The results are also sent to my email address, and can be found on any computer with ParseHub's Desktop application when I log in to my account!</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/costco_results.png" alt="How to get the locations of retail stores with web scraping" class="img-responsive" width="700px"></p></li>
</ol>

<p>There are many ways that this project can be customized. A few examples are to:</p>

<ul>
<li>Use the dropdown menu on the search results page to get the info from 20 locations instead of just 10</li>
<li>Select more data from the details page, which includes specific department phone numbers and hours</li>
<li>Use regular expressions to extract only the Monday to Friday hours, and ignore the Saturday and Sunday hours</li>
</ul>

<p>And, of course, this isn't limited to the Costco website. Just about all major retailers have store locators designed just like this one. If you use ParseHub to extract locations from maps, let us know which stores in the comments below! </p>

<p>What do you use ParseHub for? Share in the comments below or in a personal email, at quentin[at]parsehub[dot]com. We are always being surprised by the different ways people use web scraping in their daily life!</p>]]></content:encoded></item><item><title><![CDATA[ParseHub vs. Import.io Comparison – which alternative is better for web scraping?]]></title><description><![CDATA[ParseHub and import.io are two visual web scraping tools. Here is a comparison of cost, features and usability. See which suits your web scraping needs!]]></description><link>https://www.parsehub.com/blog/parsehub-vs-import-io-which-alternative-is-better-for-web-scraping/</link><guid isPermaLink="false">5a01898d-89d1-49db-9fa8-e53015a6c95e</guid><category><![CDATA[Web Scraping Tool]]></category><category><![CDATA[import.io alternative]]></category><category><![CDATA[scrapy alternative]]></category><category><![CDATA[import.io comparison]]></category><category><![CDATA[ParseHub comparison]]></category><category><![CDATA[ParseHub review]]></category><category><![CDATA[import.io review]]></category><category><![CDATA[ParseHub features]]></category><dc:creator><![CDATA[Quentin Simms]]></dc:creator><pubDate>Tue, 12 Jul 2016 19:50:24 GMT</pubDate><media:content url="/content/images/2016/07/pros_cons_parsehub_importio-1.png" medium="image"/><content:encoded><![CDATA[<img src="/content/images/2016/07/pros_cons_parsehub_importio-1.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?"><p><a href="https://www.import.io/"><strong>Import.io</strong></a> is another popular web scraping tool. The current version is entirely web based, but they have released desktop applications in the past. Here are all the similarities and differences that you need to know between the current web based import.io and the latest version of the visual web scraping tool <a href="https://www.parsehub.com/">ParseHub</a>. This comparison should be helpful if you are deciding which web scraping tool to pick. If you are currently an import.io user and want to know what your web scraping alternatives are – find out if you will benefit from switching to ParseHub below. </p>

<h1 id="cost">Cost</h1>

<p>This is probably the first comparison between import.io and ParseHub that you want to see: which web scraping software gives you the best bang for your buck? The answer is not quite clear cut, as <a href="https://parsehub.com/pricing">ParseHub's plans</a> and <a href="https://www.import.io/standard-plans/">import.io's  plans</a> have different limitations. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/price_comparison_chart-1.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="550px"></p>

<p><strong>ParseHub's plans are limited by:</strong></p>

<ul>
<li>the number of <strong>projects</strong> that can be saved</li>
<li>the number of web pages you can scrape data from per run</li>
<li>the speed at which you can collect data</li>
</ul>

<p><strong>Imoprt.io limits the number of:</strong></p>

<ul>
<li><strong>queries</strong> per month </li>
<li>the number of runs that you can store results for</li>
</ul>

<p>Most people build <strong>one ParseHub project per website</strong>, spanning over many separate web pages. Projects are <strong>not limited by the number of times they can be run</strong>.</p>

<p>On the other hand, <strong>each page</strong> extracted by import.io counts as <strong>one query</strong>, so the total number of queries you get in a month is the number of pages that can be extracted.</p>

<p>For both Import.io and ParseHub you have to pay and subscribe to a plan to get the scheduling feature – the ability to collect data from a website continuously on a schedule (daily, weekly, monthly).</p>

<p>If you don't want to learn how to use a tool and just want your data on demand, <strong>both ParseHub and Import.io offer a service that extracts data for you</strong>. Just contact sales of both companies and someone will scrape data from the website you want – delivering them in CSV/Excel or API format. </p>

<h1 id="whatcanyoudowithparsehub">What can you do with ParseHub?</h1>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/whoops_stole_Angelinas_logos.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="700px"></p>

<p>The biggest difference between ParseHub and its web scraping alternatives is that <strong>ParseHub can get data from interactive websites</strong>. </p>

<p>You can instruct ParseHub to scrape data from very complex and dynamic sites, because it can:</p>

<ul>
<li><a href="https://help.parsehub.com/hc/en-us/articles/218185307-Get-data-from-behind-a-log-in">sign in to accounts</a></li>
<li>select choices from <a href="https://help.parsehub.com/hc/en-us/articles/217752908-Click">dropdown menus</a>, radio buttons and tabs</li>
<li><a href="https://help.parsehub.com/hc/en-us/articles/218187597-Search-for-one-keyword-using-a-search-box">search with a search bar</a></li>
<li>travel to a new page simply by <a href="https://help.parsehub.com/hc/en-us/articles/217735328-Click-on-the-Next-button-to-scrape-multiple-pages-pagination-">clicking on a "next" button</a>. </li>
<li>get data from infinitely scrolling pages</li>
</ul>

<p>And more! These are all things that import.io cannot handle.</p>

<p><strong>Here is a full list of ParseHub's scraping features:</strong><!DOCTYPE html><html><head><style>table {
    font-family: arial, sans-serif;
    border-collapse: collapse;
    width: 100%;
}</p>

<p>td, th { <br />
    border: 1px solid #dddddd;
    text-align: left;
    padding: 8px;
}</p>

<p>tr:nth-child(even) { <br />
    background-color: #A4D7E4;
}
</style> <br>
</head> <br>
<body></body></html></p>

<table>  
  <tr>
    <td>Automatic IP Rotation</td>
    <td>Get data from tables and maps</td>
    <td>Conditionals and expressions</td>
  </tr>
  <tr>
    <td>Content that loads with AJAX and JavaScript</td>
    <td>Search through forms and inputs</td>
    <td>XPATH, Regular Expressions and CSS Selectors</td>
  </tr>
  <tr>
    <td>Extract text, HTML and attributes</td>
    <td>Get data from drop-downs, tabs and pop-ups</td>
    <td>REST API with web hooks</td>
  </tr>
  <tr>
    <td>Download files and images</td>
    <td>Pagination and navigation</td>
    <td>Dropbox and Google Sheets integration</td>
  </tr>
  <tr>
    <td>Scrape content from infinitely scrolling pages</td>
    <td>Scheduled Runs</td>
    <td>Regular expression selection</td>
  </tr>

</table>

<p> <br>
</p>

<p>The only downside of using ParseHub to scrape data as an alternative to import.io is that you will have to learn how to use ParseHub's <a href="https://help.parsehub.com/hc/en-us/articles/217730198-Commands-in-the-tool-box-Reference">commands</a> before doing any kind of data extraction. ParseHub's versatility is only possible because of its wide variety of commands, and that means it takes longer to learn than imoprt.io. Luckily, the application is <strong>intuitive</strong> and there are plenty of <a href="https://help.parsehub.com/hc/en-us/categories/202638628-Tutorials">tutorials</a> to help you in the help center if you get stuck!</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/07/pros_cons_parsehub_importio.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="700px"></p>

<h1 id="whatcanyoudowithimportio">What can you do with import.io?</h1>

<p>Import.io will let you extract data by learning a minimal number of commands. It can only be used on simple, or static, web pages, but some projects may take just seconds of work to build in import.io, whereas it may take you more time to build content with ParseHub.</p>

<p>Not only do the web pages need to be simple, but the website URLs need to be as well. See <strong>import.io pagination</strong> for more information on this.</p>

<p>When a new project is started, import.io tries to guess what you want from the page. They used to call this feature Magic extraction. For very simple websites, this usually provides a good starting place for the project and could cut down the time to build an extractor down to just a few seconds.</p>

<h1 id="whatcanyoudowithboth">What can you do with both?</h1>

<p>Both web scraping tools will let you download extracted data as CSV's and as JSON. Both will let you import data directly into a Google Sheet and can be <strong>controlled as an API</strong>. You can schedule runs with import.io and with ParseHub, too, depending on your payment plan.</p>

<p>When building your project, you can get <strong>real time feedback</strong> from ParseHub's sample and test runs, or from import.io's data view. Both web scrapers allow you to use <strong>regular expressions</strong> to scrape only the text that you want.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/UI_comparison_chart.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="700px"></p>

<p>You create <a href="http://importio.desk.com/customer/en/portal/articles/2375940-what-is-the-import-io-extractor-?b_id=12993"><strong>extractors</strong></a> to extract data with import.io, and <a href="https://help.parsehub.com/hc/en-us/articles/218180927-ParseHub-basics"><strong>templates</strong></a> to extract data with ParseHub. These are very similar: they are both sets of instructions that the software uses to select and extract items from a webpage. </p>

<p>However, an import.io user doesn't enter the commands directly. Instead, they <a href="http://importio.desk.com/customer/en/portal/articles/2375947-building-an-extractor?b_id=12993#websiteview"><strong>train</strong></a> the extractor to select and extract what they want from a page. On the other hand, a ParseHub user enters <a href="https://help.parsehub.com/hc/en-us/articles/217753218-What-is-a-command-"><strong>commands</strong></a> directly into the template. When ParseHub executes the template, it follows the instructions exactly as entered. When import.io executes a template, it remembers what the user trained it to do, and executes the same way.</p>

<p><strong><em>This is an example ParseHub project. You can see all of the different commands that the user entered, like Select, Hover, and Extract, in the left side bar.</em></strong></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/parsehub_example_billboard.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="700px"></p>

<p><strong><em>Training the import.io extractor takes place in their "website view", as seen above.</em></strong></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/importio_example_kijiji.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="700px"></p>

<h3 id="scrapingcommands">Scraping Commands</h3>

<p>You can only train import.io to <strong>select and extract</strong> items on a web page. Meanwhile, ParseHub has a <a href="https://help.parsehub.com/hc/en-us/sections/203980687-Commands">wider variety of commands</a> that allow a user to navigate through web pages and deal with interactive elements. </p>

<p><strong>List of ParseHub commands that can be combined to perform advanced scraping functions:</strong></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/parsehub_commands_menu.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="200px"></p>

<h3 id="parsehubpagination">ParseHub pagination</h3>

<p>Because of the variety of commands available, <strong>ParseHub can navigate to different pages a variety of ways, including <a href="https://help.parsehub.com/hc/en-us/articles/217752908-Click">clicking on links</a> and by entering text into <a href="https://help.parsehub.com/hc/en-us/articles/217753298-Input">search boxes</a></strong>. Remember that one ParseHub project may have several very different templates all working together, with the end goal of getting you a complete set of data.</p>

<h3 id="importiopagination">Import.io pagination</h3>

<p>Since you can't command an import.io extractor to navigate to different pages through clicking or searching, this means that page navigation is only possible by <a href="http://importio.desk.com/customer/portal/articles/2378756-adding-urls-to-your-extractor">entering a list of pages</a>. When you run an import.io extractor, it will run on all of the URLs that you enter in this list. Each URL scraped counts as one <strong>query</strong>. Pages with different structures require building different extractors.</p>

<p>If you are able to identify a pattern in the URLs of the pages you want to run an extractor on, like "/page=1", "/page=2", etc., then import.io has a tool that will help so you don't have to manually copy and paste each one. <a href="http://importio.desk.com/customer/en/portal/articles/2435553-url-generator?b_id=12993">The URL generator</a> will give you a list of URLs to run your extractor on.</p>

<h1 id="webscrapingexample">Web Scraping Example</h1>

<p>I tested out both import.io and ParseHub on a popular website – Amazon to see how they compare. First let's have a look at import.io and then go to ParseHub.</p>

<p><strong>Problem: get pricing details for every single watch sold by one of Amazon's "Top Brands".</strong> </p>

<h2 id="importioproject">import.io project</h2>

<h3 id="firstextractor">First extractor</h3>

<p>The list of watch brands in amazon.ca's Top Brands was found at <a href="https://www.amazon.ca/gp/search/other/ref=sr_sa_p_89?rh=n%3A2235620011%2Cp_n_target_audience_browse-bin%3A2305364011%2Cp_6%3AA3DWYIK6Y9EEQB&amp;bbn=2235620011&amp;sort=review-rank&amp;pickerToList=lbr_brands_browse-bin&amp;ie=UTF8&amp;qid=1466796550">this url</a>. I want to scrape information from each one of the watch brands listed here. import.io will be able to do this if I have a list of each one the URLs. I will create an extractor to scrape this page and get this list.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/1--list-of-brands.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="700px"></p>

<p><strong>When you start a new extractor in import.io, it first tries to guess what you are trying to extract on the page.</strong> For this particular web page, it took about 10 seconds to load a table with the data it thought I wanted.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/3_importio_first_try.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="700px"></p>

<p>After looking over the software's first guess, you then turn to the <strong>website view</strong> tab to <strong>refine your training</strong>. In this case, import.io only selected the first column of brands, and I wanted all three columns, so I had to train the extractor to do that.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/importio_first_website_view.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="700px"></p>

<p>Unfortunately, when I clicked on the other brand names, the software didn't recognize that I wanted to select them all and so only selected one at a time. Instead of clicking on 34 more brands, I decided to delete the already extracted columns and start from scratch.</p>

<p>I clicked on three brands, the first, middle and last, and then import.io automatically selected all of the others.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/5_fixing_importio_1.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="600px"></p>

<p>I was happy with the data view so I ran the extractor and downloaded the CSV file of the links. It didn't look exactly like the data I saw on the website: there are three columns instead of just one with the URLs, like I wanted.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/6_first_csv.png
" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="700px"></p>

<p>In old versions of import.io it was possible to "chain" extractors together so that the links extracted by one would automatically be sent to the other. However, it does not seem like it is possible to do this with the current version without any manual entering or without creating a script using the import.io API.</p>

<h3 id="secondextractor">Second extractor</h3>

<p>I trained the next extractor on one of the links I got from the CSV given to me from the first extractor. I chose a brand that had lots of different watches so that there was more than one page of results. This time, import.io's initial guess did select all the watches on the page and required less additional training than the first one.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/7_importio_second_extractor_first_guess.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="700px"></p>

<p>Back in website view, I trained the extractor to select and extract exactly what I wanted. Lastly, I used the regular expressions option to <a href="http://importio.desk.com/customer/en/portal/articles/2474136-regular-expression-find-and-replace?b_id=12993">remove the unwanted "CDN$" prefix</a> that was in front of all the prices.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/9_fixing_second_extractor.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="700px"></p>

<p>In total, I was able to complete the training for this extractor fairly quickly, in 4 or 5 minutes. I now had to input links to all of the different pages that I wanted to be included in my query. <strong>This was a problem that I didn't know how to resolve.</strong></p>

<p>import.io's URL generator only helps generate URLs if the pages follow an obvious pattern. On amazon.ca, they do not. The page URLs are all very long, and there seems to be very little correlation between the URL of page 1 and page 2 of the same brand. Additionally, I had to copy and paste the links column from the CSV into a word document and add a line break at the end of each URL manually in order to put the list into the second extractor - I wasn't able to find an easier way to do this.</p>

<p><strong>Pros:</strong> </p>

<ol>
<li>Training both templates took a very short amount of time - most columns of data required 2 or 3 clicks so it took only minutes to select everything that was needed.  </li>
<li>Regular expressions were able to remove the unwanted prefixes before exporting the data</li>
</ol>

<p><strong>Cons:</strong></p>

<ol>
<li>import.io failed to travel to every page because there was no pattern to amazon.ca's URLs. Instead of every single watch, like I had wanted, I was only able to scrape the first page of each brand.  </li>
<li>There was no easy way to "chain" the extractors together, and that meant I had to manually enter line breaks after each URL that was scraped. I would have had to write a script to do this for me if there had been hundreds or thousands of URLs - which isn't practical for someone who is using the tool because they don't know how to code!  </li>
<li>Columns of unwanted and unexpected data were extracted.</li>
</ol>

<h2 id="parsehubproject">ParseHub project</h2>

<h3 id="maintemplate">Main template</h3>

<p>The ParseHub project started on the <a href="https://www.amazon.ca/gp/search/other/ref=lp_7012516011_sa_p_89?rh=n%3A2235620011%2Cn%3A!2235621011%2Cn%3A7012516011&amp;bbn=7012516011&amp;pickerToList=lbr_brands_browse-bin&amp;ie=UTF8&amp;qid=1467305254">same page</a> as the import.io project. It took 2 clicks to select all of the bands on the page, and then ParseHub was told to click on each one of them and go to the next template, called <strong>brand</strong>.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/11_main_template.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="700px"></p>

<h3 id="brandtemplate">Brand template</h3>

<p>To create this template, I travelled to the first page of one of the brands that had lots of watches, so that I could make ParseHub travel to multiple pages of results.</p>

<p>Clicking on the names of two of the watches caused all of them to be selected, and then I used the Relative Select tool to select and extract all of the other information that I wanted. Just like with the import.io project, I was able to use <a href="https://help.parsehub.com/hc/en-us/articles/217736078-Regular-Expressions">regular expressions</a> to remove the unwanted prefix before the prices.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/12_brand_template.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="700px"></p>

<p>I added one last Select command to travel to the next page of results using the "Next" button. Once ParseHub clicks on it, it will execute the template again and extract all the data that I instructed it to. </p>

<p>The "Next" button on the final page of results does not lead to a new page, so I used <a href="https://help.parsehub.com/hc/en-us/articles/220618167-Using-XPath-to-Select-Elements">XPath selection</a> in order to select the button <em>only</em> when it is linked to a new page. That way, ParseHub will not start this template again after the final page, and will instead go to the next brand in the main_template list.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/13_next_button.png" alt="ParseHub vs. Import.io Comparison – which alternative is better for web scraping?" class="img-responsive" width="700px"></p>

<p><strong>Pros:</strong> </p>

<ol>
<li>ParseHub handled the amazon.ca pagination very well. It was able to travel to each brand in the list, and also able to travel to every page within a brand.  </li>
<li>Regular expressions removed unwanted prefixes.  </li>
<li>Data columns were exactly what was needed, no more and no less.  </li>
<li>ParseHub's navigation tools make it easy to add to this project and get even more information about the watches: you can click on each watch, or hover over them, to reveal more details.</li>
</ol>

<p><strong>Cons:</strong></p>

<ol>
<li>XPath selection is an advanced technique that would be difficult for a beginner user to use.</li>
</ol>

<h3 id="finalthoughts">Final thoughts</h3>

<p>It is very quick and easy to put together an import.io extractor. On many of the websites that I have tested, the "magic extractor"'s initial guess works almost exactly how you want it to, meaning you can start running your extractor just a minute or two after creating it. But it doesn't matter how quickly the web extractor is trained, it is frankly severely <strong>limited by the way it navigates through pages</strong>. I wasn't able to complete the project I used as an example here because of this limitation, I chose a simple and realistic use case. </p>

<p>I found that the documentation for this version of import.io was also limiting, especially compared to all of the videos and tutorials that I found for the older versions. This could have been the reason that I was unable to find a way to chain the two extractors together: <strong>if someone with more experience with import.io can explain, either in a personal email or in the comments below, how to solve the problem I was having with the example project, then I would be happy to edit this comparison</strong>. But as of now, it seems impossible with this version of the software.</p>

<p>Completing my ParseHub project took between 10 and 15 minutes in total. It took only a few minutes to train each import.io extractor, but I spent far more than 15 minutes trying to figure out how to get them to travel to every page. For this reason, I spent less time total working with ParseHub than I did with import.io.</p>]]></content:encoded></item><item><title><![CDATA[Scrape data from tables & convert currencies to USD automatically]]></title><description><![CDATA[<p>The team behind <a href="https://www.parsehub.com/">ParseHub</a> – a web scraping tool – just finished researching the cost of public and private <a href="https://www.parsehub.com/blog/the-outrageous-cost-of-university-tuition-in-40-countries/">University tuition in 40 countries</a>. We used web scraping to collect statistics from three different tables that had economic data – the monthly living wage, the monthly low skilled worker wage and minimum hourly</p>]]></description><link>https://www.parsehub.com/blog/scrape-data-from-tables-convert-currencies-to-usd-automatically/</link><guid isPermaLink="false">660880dd-5e10-4687-a934-ddcdeadd065a</guid><category><![CDATA[Data Fun]]></category><category><![CDATA[Data Science]]></category><category><![CDATA[Web Scraping]]></category><category><![CDATA[How To Guides]]></category><category><![CDATA[Tableau]]></category><category><![CDATA[ParseHub]]></category><category><![CDATA[Data in tables]]></category><category><![CDATA[Currency conversion]]></category><dc:creator><![CDATA[Angelina Fomina]]></dc:creator><pubDate>Wed, 22 Jun 2016 18:55:44 GMT</pubDate><content:encoded><![CDATA[<p>The team behind <a href="https://www.parsehub.com/">ParseHub</a> – a web scraping tool – just finished researching the cost of public and private <a href="https://www.parsehub.com/blog/the-outrageous-cost-of-university-tuition-in-40-countries/">University tuition in 40 countries</a>. We used web scraping to collect statistics from three different tables that had economic data – the monthly living wage, the monthly low skilled worker wage and minimum hourly wage. Automating the process of data extraction saved us hours that we would have spent coping and pasting if we did not have a quick data collection tool available.</p>

<p>After setting up a web scraping desktop app to collect all of the data, we downloaded the web data in Excel and cleaned it up. During the process, we used regular expressions within the scraping app to clean up how the text would appear in our Excel file. </p>

<p>Lastly, we imported the scraped data into <a href="http://get.tableau.com/">Tableau</a> and visualized several bar graphs and world maps (like the one you see below).</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/Can-students-afford-public-University-by-working-while-studying-.png" alt="paying for public university tuition while studying" width="600px"></p>

<h3 id="whatisthestateofuniversitytuitioncostsaroundtheworld">What is the state of University tuition costs around the world?</h3>

<p>We found out that <a href="https://www.parsehub.com/blog/students-cant-afford-to-live-and-pay-for-university-tuition-in-22-countries-2/">students can't afford their living expenses and tuition costs</a> by working at minimum wage while studying. </p>

<p>However, students in 83% of the countries we looked at can afford to pay off their public University tuition by working during their studies. Students will need to find alternative funding to be able to afford both living expenses and tuition at the same time. This could explain why the <a href="http://blogs.wsj.com/economics/2015/11/23/more-young-adults-live-with-their-parents-now-than-during-the-recession/">number of young adults living with their parents is increasing</a> – to save on living expenses. </p>

<p>Students are more likely to take out loans if they can't rely on their parents for support. This would explain why the <a href="http://blogs.wsj.com/economics/2015/05/08/congratulations-class-of-2015-youre-the-most-indebted-ever-for-now/">average debt per graduate in the United States is $35,051</a> (2015).</p>

<h3 id="inthistutorialyouwilllearnhowto">In this tutorial you will learn how to:</h3>

<p><strong>Part 1: Using a web scraping tool</strong></p>

<ol>
<li>Scrape data from a website that displays wage statistics in a table with a web scraping tool.  </li>
<li>Clean up text on webpage with regular expressions.  </li>
<li>Set up ParseHub to automatically scrape the local currency and run the numbers through a <a href="https://ca.finance.yahoo.com/currencies/converter/#from=CAD;to=USD;amt=1">currency converter</a> to get all of the wages in USD.</li>
</ol>

<p><strong>Part 2: Using Tableau to visualized scraped data</strong></p>

<ol>
<li>Clean up the data in Excel, in preparation for <a href="http://get.tableau.com/">Tableau</a>.  </li>
<li>Create a bar graph in Tableau to display your data.  </li>
<li>Make a map visualization from the data you scraped with ParseHub.</li>
</ol>

<h1 id="1useparsehubtoscrapestatisticsfromtables">1. Use ParseHub to scrape statistics from tables</h1>

<p>First, we will to set-up ParseHub – a web scraping tool – to automatically collect statistics from two tables:</p>

<ul>
<li><a href="http://www.tradingeconomics.com/united-states/wages-low-skilled">Trading Economics - Low Skilled Wages</a></li>
<li><a href="http://www.tradingeconomics.com/united-states/living-wage-individual">Trading Economics - Living Wages</a></li>
</ul>

<p>Now, you don't need to copy and paste data from the two tables manually. Instead of hiring someone to copy and paste data for you – you can save money by setting up a web scraping application to collect the data for you. As a result all of the statistics will be scraped into an Excel file for you.</p>

<h2 id="step1openupparsehubtheeconomicswebsite">Step 1: Open up ParseHub &amp; the Economics Website</h2>

<ol>
<li>Go to <a href="https://www.parsehub.com/quickstart">ParseHub.com</a> to download the web scraping desktop up and sign up.  </li>
<li>Open the desktop application and click "New Project".  </li>
<li>In the text box on the next page enter the following url – <em><a href="http://www.tradingeconomics.com/united-states/wages-low-skilled">http://www.tradingeconomics.com/united-states/wages-low-skilled</a></em> – this website contains the statistics tables for wages in 40 different countries.  </li>
<li>Click "Start project on this URL". You will be taken to the main_template of ParseHub where you will put in instructions to scrape this website. The website will also automatically open for you.</li>
</ol>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/enter_economics_website.png" alt="Scrapes wages from tables" width="300px"></p>

<h2 id="step2setupwebscrapertoextractallofthedataintoanexcelfile">Step 2: Set-up web scraper to extract all of the data into an Excel file</h2>

<h3 id="scrapeallofthecountrynamesandtheirurls">Scrape all of the country names and their urls:</h3>

<ol>
<li>Scroll down to to the table that has the low skilled wages for all of the countries.  </li>
<li>Click on the first country that you see "Angola"  </li>
<li>Now, click on the second country. All of the countries should be selected for you in green. Rename the selection1 command to "countries". </li>
</ol>

<p><strong>That is it! All of the countries and the url for the countries profile is now extracted for you</strong> If you click on the "CSV/Excel" blue button in the bottom right corner of the browser you will be able to see how the results will look like in Excel.</p>

<p><em>View of the ParseHub side-bar:</em></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/sidebar_parsehub_scraping_statistics.png" alt="Scrapes wages from tables" width="300px"></p>

<p><em>View of the entire web scraping desktop app:</em></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/view_entire_webscraping_app_for_scraping_statistics.png" alt="Scrapes wages from tables" width="600px"></p>

<h3 id="scrapethewagesforallofthecountriesandthecurrencytype">Scrape the wages for all of the countries and the currency type.</h3>

<p><strong>Collect the wages for each country:</strong></p>

<ol>
<li>Click on the <strong>plus button</strong> next to the "Begin new entires in countries" command to make another selection in the table.  </li>
<li><p>From the menu that pops out choose the "Relative Select" tool. This tool will let you create a relationship between one data point on the table and another. It will also make sure that data points that are similar show up in the same row in Excel. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/choose_the_select_tool_scraping_statistics.png" alt="Scrapes wages from tables" width="600px"></p></li>
<li><p>Click on one country – for example Angola.  </p></li>
<li>Now click on the wage for that country in the next column. An arrow should appear between all of the countries and all of the wages. Rename the "Relative selection" command to wages.</li>
</ol>

<p><strong>All Done! ParseHub will now scrape all of the wages for each country into your Excel file</strong></p>

<p><em>View of the ParseHub side-bar:</em></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/parsehub_sidebar_view_scrape_wages.png" alt="Scrapes wages from tables" width="300px"></p>

<p><em>View of the entire web scraping desktop app:</em></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/selecting_and_scraping_data_from_a_table-1.png" alt="Scrapes wages from tables" width="600px"></p>

<p><strong>Collect the currency for each country:</strong></p>

<p>Let's repeat the steps above to scrape the currencies for 50 countries that we are getting wages for. We will need this information to enter it into a currency converter later.</p>

<ol>
<li>Click on the <strong>plus button</strong> of the "Begin new entry in countries" command.  </li>
<li>From the menu that pop-ups choose the "Relative Select" tool again so you can select another data point that relates to the countries in the table.  </li>
<li>Click on the first country.  </li>
<li>Click on the currency in the 4th table – for example "AOA/Month". All of the currencies for all of the countries should be highlighted for you in green. Rename the "Relative select" command to "currency"</li>
</ol>

<p><em>View of the ParseHub side-bar:</em></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/view_of_sidebar_for_scraping_currencies_parsehub.png" alt="Scrapes wages from tables" width="300px"></p>

<p><em>View of the entire web scraping desktop app:</em></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/view_of_parsehub_web_scraping_app_for_scraping_currencies.png" alt="Scrapes wages from tables" width="600px"></p>

<h1 id="2cleanuptextwithregularexpressions">2. Clean up text with Regular Expressions</h1>

<p>ParseHub has regular expressions built into it. You can use this more advanced feature to clean up your data before it even appears in your Excel file. This takes away one cleaning step you will have to do with your data after downloading the Excel file.</p>

<p>In this example the currency in the table is displayed as <strong>EUR/Month</strong>. We want to take away the <strong>/Month</strong> portion and have it displayed as <strong>EUR</strong>.</p>

<ol>
<li>Click on the <strong>plus button</strong> on the "Relative currency" command.  </li>
<li>On menu that pops up click on "Advanced" to display more tools.  </li>
<li><p>Choose the "Extract" tool. This tool will let you display the actual extraction for the "Relative currency" command and you will be able to modify it.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/scraping_currencies_parsehub.png" alt="Scrapes wages from tables" width="600px"></p></li>
<li><p>In the bottom panel that appears under the "Extract currency" command click on the "Use regex" option.  </p></li>
<li>In the "regex" text box type <strong>(.*)/Month</strong>. This will let you scrape cleaned up numbers without the /Month.</li>
</ol>

<p><em>View of the ParseHub side-bar:</em></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/scraping_statistics_regular_expressions_parsehub.png" alt="Scrapes wages from tables" width="300px"></p>

<p><em>View of your sample results:</em></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/sample_results_with_regular_expressions.png" alt="Scrapes wages from tables" width="600px"></p>

<p>Notice how the "countries_currency" table was cleaned up to match the regular expression on the currency command.</p>

<h1 id="3automaticallyconvertlocalcurrenciestousdin10minutes">3. Automatically convert local currencies to USD in 10 minutes</h1>

<p>Now, we will set-up ParseHub to input the data scraped from the statistics tables into a currency converter. As a result you will be able to download an Excel file with all of the wages from 40 countries converted into USD.</p>

<p>You don't have to waste hours manually converting the wages from their local currency to USD for a research project. You can use a web scraper to automate the task of conversion for you. In this example, we will use ParseHub to automatically fill out search forms and to click on drop-downs in order to convert currency.</p>

<h2 id="step1setupwebscrapertonavigatefromonewebsitetoanotherwebsite">Step 1: Set up web scraper to navigate from one website to another website</h2>

<ol>
<li>Click on the <strong>plus button</strong> of the "Begin new entry in countries command.  </li>
<li>Click on the "Advanced" text in the pop-up menu that appears to display more tools.  </li>
<li><p>Choose the "Go To Template" tool from the menu. This tool will let you enter the url of a new website that you want to navigate too.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/choose_the_go_to_template_tool_from_menu.png" alt="Scrapes wages from tables" width="600px"></p></li>
<li>In the pop-up box that appears enter the following url in single quotes into the "Go to URL" text box – <em>'<a href="https://ca.finance.yahoo.com/currencies/converter/#from=CAD;to=USD;amt=1">https://ca.finance.yahoo.com/currencies/converter/#from=CAD;to=USD;amt=1</a>'</em>  </li>
<li>In the pop-up box also type in the name of the new template in the text box – "currency".  </li>
<li>Click "Create New Template". This will tell ParseHub to create a new template to store all of the instructions for the new website that you want to navigate to.</li>
</ol>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/pop_up_menu_going_to_new_template.png" alt="Scrapes wages from tables" width="600px"></p>

<h2 id="step2setupwebscrapertoautomaticallyfilloutsearchformsandgrabresults">Step 2: Set up web scraper to automatically fill out search forms and grab results</h2>

<ol>
<li><strong>Make sure that your second template allows for duplicates</strong> first.  </li>
<li><p>Click on the "Options" dropdown from the "currency" template. Click on the "No Duplicates" option and make sure it is de-selected. This will make sure that ParseHub will visit the currency conversion website multiple times.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/no_duplicates_found_parsehub.png" alt="Scrapes wages from tables" width="300px"></p></li>
<li><p>Click on the text box that currently displays "Canadian Dollar CAD" to input the type of currency that you want to convert to USD. This will select it and add an Input command.  </p></li>
<li>In the Input text box type in <strong>currency</strong>.  </li>
<li>From the "Input type" dropdown box select "expression".  </li>
<li><p>Rename the "selection1" to "search".</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/input_value_to_search_box_parsehub.png" alt="Scrapes wages from tables" width="300px"></p></li>
<li>Make sure the search command is selected. Click on the "Wait up to 60 seconds for element to appear" checkbox. ParseHub will wait to make sure that the input box appears before proceeding to the rest of the instructions.</li>
</ol>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/wait_for_element_to_appear_on_input_textbox.png" alt="Scrapes wages from tables" width="300px"></p>

<ol>
<li>Click on the plus button of the "Select page" command.  </li>
<li>From the menu choose the Select tool.  </li>
<li>Click on the amount search box. It should be selected in green for you. Rename the Select &amp; Extract selection1 command to "amount".  </li>
<li>Click on the "Wait for up to 60 seconds for elements to appear" text box.  </li>
<li>ParseHub did not automatically detect that an input box was selected so you have to tell ParseHub to input text by choosing the input tool.  </li>
<li>Click on the "Select &amp; Extract selection1" command plus button.  </li>
<li>On the menu click on the "Advanced" text box to open up the menu.  </li>
<li><p>Choose the input tool from the extended menu.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/choose_input_tool_from_menu.png" alt="Scrapes wages from tables" width="300px"></p></li>
<li>Type in "wage" into the input box.  </li>
<li><p>From the drop-down select "expression".</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/Screenshot-2016-06-21-20-33-18.png" alt="Scrapes wages from tables" width="300px"></p></li>
<li>Click on the <strong>plus button</strong> of the Select page command and choose the select tool from the menu.  </li>
<li>Click on the text box on the right that shows the converted amount. Rename the "Select &amp; Extract" node to converted amount.  </li>
<li>Click on the "Wait up to 60 seconds for elements to appear" check-box. </li>
</ol>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/get_converted_amount_with_parsehub_automatically.png" alt="Scrapes wages from tables" width="600px"></p>

<h2 id="step3runparsehubtoscrapestatisticsdownloaddata">Step 3: Run ParseHub to scrape statistics &amp; download data</h2>

<ol>
<li>Click on the green "Get Data" button.  </li>
<li>Click "Run"  </li>
<li>Click "Save and Run" one more time</li>
</ol>

<p>Your results will appear on this page and will be available for download via Excel or JSON when ParseHub is done scraping all of the pages. Alternatively, you will get an email with the link to download your results.</p>

<h2 id="sources">Sources:</h2>

<ul>
<li><a href="https://ca.finance.yahoo.com/currencies/converter/#from=CAD;to=USD;amt=1">Currency converter - Yahoo</a> - all costs were converted into USD.</li>
<li>Trading Economics and  <a href="http://www.wageindicator.org/main/salary/living-wage/living-wage-map">Wage Indicator</a> were used to find the <a href="http://www.tradingeconomics.com/united-states/wages-low-skilled">low skilled wages</a>,
<a href="http://www.tradingeconomics.com/united-states/minimum-wages">minimum wages</a> and <a href="http://www.tradingeconomics.com/united-states/living-wage-individual">individual living wages</a>.</li>
<li><a href="https://www.parsehub.com/">ParseHub</a> a web scraping tool was used to collect data easily and quickly.</li>
<li><a href="http://www.tableau.com/">Tableau</a> was used to analyze and visualize information.</li>
<li>A variety of sources were used to calculate the private and public tuition rates for Universities - <a href="http://www.oecd-ilibrary.org/docserver/download/9615031e.pdf">1</a>, <a href="http://gse.buffalo.edu/org/inthigheredfinance/project_profiles.html">2</a>,
<a href="http://siteresources.worldbank.org/EDUCATION/Resources/278200-1099079877269/547664-1099079956815/wps4517.pdf">3</a>.</li>
</ul>

<div class="text-center">  
<h4>Continue Reading...</h4>  
<h3>Use Tableau to Visualize Scraped Data</h3>  
<p>coming soon!</p>  
</div>]]></content:encoded></item><item><title><![CDATA[Students can't afford to live and pay for University tuition in 22 countries]]></title><description><![CDATA[<p>In most countries in the world, students simply cannot afford to pay for their living expenses and their University tuition by working while in school. The ability to get a higher education is still strictly dependent on their family – now more than ever.</p>

<p>If you are born to supportive, middle-class</p>]]></description><link>https://www.parsehub.com/blog/students-cant-afford-to-live-and-pay-for-university-tuition-in-22-countries-2/</link><guid isPermaLink="false">7fe951b0-07fd-495a-9cfa-78dcf9ba9f26</guid><category><![CDATA[Data Fun]]></category><category><![CDATA[Data Science]]></category><category><![CDATA[Data Scraping]]></category><category><![CDATA[Web Scraping]]></category><category><![CDATA[Education]]></category><category><![CDATA[University]]></category><category><![CDATA[Tuition costs]]></category><category><![CDATA[Statistics]]></category><category><![CDATA[Economics]]></category><category><![CDATA[Web Scraping Tool]]></category><category><![CDATA[Data Extraction]]></category><dc:creator><![CDATA[Angelina Fomina]]></dc:creator><pubDate>Tue, 21 Jun 2016 19:59:20 GMT</pubDate><content:encoded><![CDATA[<p>In most countries in the world, students simply cannot afford to pay for their living expenses and their University tuition by working while in school. The ability to get a higher education is still strictly dependent on their family – now more than ever.</p>

<p>If you are born to supportive, middle-class parents that worked hard to save for your education – you have won the education lottery. In most countries, the money that your hypothetical parents have saved will be able to cover your tuition costs and some living expenses. If your parents provide you with food and a place to live, then, in most cases, you already have almost enough saved to get a University degree. You can take on a part-time job, for minimum wage, while studying and pay off the missing difference.</p>

<p>However, if you don't have anyone to rely on at all, you are left at the mercy of loan officers and at government funded programs – if your country has them.</p>

<p>It is very hard to make it completely on your own. Even with grit, hard-work and plenty of dedication, a student with no support will have a difficult time getting a University degree, in most countries.</p>

<h2 id="canstudentsaffordlivingandtuitioncostsbyworkingwhileinschool">Can students afford living and tuition costs by working while in school?</h2>

<p>Let's say you had to live alone, provide for your basic living expenses and also pay money to your University every year. Is it possible for you to cover all of your expenses by working at minimum wage?</p>

<blockquote>
  <p>In most counties, working while studying won't help you cover all of your tuition and basic living expenses.</p>
</blockquote>

<p>As a student you would have 3 summer breaks where you can work full time during a 4 years program. You can also, with a lot of dedication, pull off a 20 hour work week during the school year. Taking this into account, we calculated how much money you would still have to make on top of your minimum wage job to afford living and education.</p>

<p>Getting a well paying job isn't always the option for most students, because employers want you to have a degree before they hire you. The extra money will have to come from somewhere – loans, scholarships, generous family members or by taking time off between high school and University to save up. </p>

<h3 id="payingforpublicuniversitytuitionandbasiclivingexpenses">Paying for public University tuition and basic living expenses:</h3>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/Can-students-afford-living-expenses-and-public-tuition-by-working-while-studying-.png" alt="paying for living expenses and public tuition costs" width="600px"></p>

<p>As a student, it is only possible for you to support yourself and pay off your University tuition if you were studying in Sweden and Finland. Both of the countries have free University education, high low-skilled monthly wages and minimum wages.</p>

<p><strong>In the 22 countries we studied, you will have to find alternative sources to help fund your education and living costs.</strong></p>

<blockquote>
  <p>If you are a student in the United States, you will need to find $74,840 in alternative funding to support yourself for 4 years and to pay for your tuition.</p>
</blockquote>

<p>This would explain why – "About <a href="http://www.marketwatch.com/story/americas-growing-student-loan-debt-crisis-2016-01-15">40 million Americans have student loans</a> and about 70% of bachelor’s degree recipients graduate with debt" – MarketWatch</p>

<p><strong>The <a href="http://blogs.wsj.com/economics/2015/05/08/congratulations-class-of-2015-youre-the-most-indebted-ever-for-now/">average debt per students</a> of the 2015 graduating class in the United States is $35,051.</strong></p>

<p>Students that want to study in a public University in the US will need to somehow find $74,840 to support themselves and afford University. If an American student works part-time during school and full-time during the summer, at a minimum wage job, they will only be able to make $46,000. </p>

<p>The United States is the worst for it's lack of affordable education if you look at the savings required to get a University degree. However, other developed counties are not far behind. The <a href="http://cfs-fcee.ca/wp-content/uploads/sites/2/2015/03/Report-Impact-of-Student-Debt-2015-Final.pdf">average student in Canada graduates with $21,8886</a> in student debt and with <a href="http://www.dailymail.co.uk/news/article-3562667/Graduates-average-44-000-debt-Students-leaving-English-universities-owe-world-following-tuition-fee-hikes.html">$20,900 in Australia</a>. Surprisingly, <a href="http://www.dailymail.co.uk/news/article-3562667/Graduates-average-44-000-debt-Students-leaving-English-universities-owe-world-following-tuition-fee-hikes.html">English students on average owe $64,600</a> – the most when compared to the US, Canada and Australia. </p>

<h3 id="payingforprivateuniversitytuitionandbasiclivingexpenses">Paying for private University tuition and basic living expenses:</h3>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/Can-students-afford-living-expenses-and-private-tuition-by-working-while-studying-.png" alt="paying for living expenses and public tuition costs" width="600px"></p>

<p>Taking a look at private University education, we can see an even bigger difference between the United Stated and every other country we studied. You will need 2 times more money to go to private school in the US, than the 2nd "most expensive" country Brazil.</p>

<h2 id="canstudentspayforonlytheirtuitionwhilestudying">Can students pay for only their tuition while studying?</h2>

<p>We know that in most countries you won't be able to afford both your living expenses and tuition by working part-time during the school year and full-time during the summer. </p>

<p>Can you at least afford just tuition by working a total of 5020 hours at minimum wage? </p>

<blockquote>
  <p>In 83% of the countries you can pay off your public University degree by working while studying.</p>
</blockquote>

<p>The only thing you need is: supportive parents. You don't need your parents to hustle away at your education fund. You just need them to keep a roof over your head close to a University and to feed you. The rest you can do on your own.</p>

<p>This would again explain why students are more likely to live with their parents now then ever before. <a href="http://cfs-fcee.ca/wp-content/uploads/sites/2/2015/03/Report-Impact-of-Student-Debt-2015-Final.pdf">42% of Canadians under the age of 30</a> still live with their parents - that is up 15% from 1981. <strong>In the United States, the amount of <a href="http://blogs.wsj.com/economics/2015/11/23/more-young-adults-live-with-their-parents-now-than-during-the-recession/">18 to 34 year olds living with their parents</a> was 31.5% in 2015.</strong> This percentage continuously increased from 27% in 2005.</p>

<p>Living with parents not only helps students afford to go to University, but also helps them pay off their student loans after graduation.</p>

<h3 id="payingoffpublicuniversitytuitionwhilestudying">Paying off public University tuition while studying</h3>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/Can-students-afford-public-University-by-working-while-studying-.png" alt="paying for public university tuition while studying" width="600px"></p>

<p>You can even pay off public University tuition while studying in the United States. The only countries you may struggle a little bit are the Philippines, Russia, Hong Kong and Chile – most likely because the minimum wage remains low.</p>

<h3 id="payingoffprivateuniversitytuitionwhilestudying">Paying off private University tuition while studying:</h3>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/Can-students-afford-private-University-by-working-while-studying-.png" alt="paying for private university tuition while studying" width="600px"></p>

<blockquote>
  <p>It is impossible to save for private education while studying in almost half of the countries. Private Universities are the least financially attainable in the United States.</p>
</blockquote>

<p>Looking at exactly how much students have to save up or the amount of cash they need to borrow should make you stop and think. Education is much harder to attain without family support – now more then ever before. This will have a huge impact on our society. Students that graduate with debt will take longer to get up on their own two feet. They will continue living with their parents and potentially diving into their parents retirement savings while looking for support. The new generation of graduates will have a harder time saving up for a home, taking care of their aging parents and overall contributing back to society.</p>

<p>Getting a University degree is still not readily available to everyone – even though almost every well paying job requires a University degree. Higher education largely remains a luxury and will require a student from a lower class family to work extremely hard and to fall financially behind.</p>

<p><strong>There's more!</strong></p>

<ul>
<li><a href="https://www.parsehub.com/blog/the-outrageous-cost-of-university-tuition-in-40-countries/">The outrageous cost of University tuition in 40 countries</a></li>
<li>Scrape and clean data automatically for visualizing in Tableau</li>
</ul>]]></content:encoded></item><item><title><![CDATA[Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?]]></title><description><![CDATA[<h3 id="forgetabouttheweathersohowaboutthattrumpguyhasbecomethepreferredwayofstartingsmalltalkforpeoplearoundtheglobe">Forget about the weather, "So, how about that Trump guy?" has become the preferred way of starting smalltalk for people around the globe.</h3>

<p>The US Presidential Primaries have been the world’s biggest news sensation for the past 12 months, and the coverage will only increase now that both major</p>]]></description><link>https://www.parsehub.com/blog/what-27000-tweets-can-tell-you-about-the-presidential-primaries/</link><guid isPermaLink="false">18e826eb-578c-4b6e-b6f3-b420ae4890e4</guid><category><![CDATA[Data Fun]]></category><category><![CDATA[Twitter]]></category><category><![CDATA[Sentiment Analysis]]></category><category><![CDATA[media coverage]]></category><category><![CDATA[Donald Trump]]></category><category><![CDATA[Hillary Clinton]]></category><category><![CDATA[Bernie Sanders]]></category><category><![CDATA[Web Scraping]]></category><category><![CDATA[Data Science]]></category><category><![CDATA[python]]></category><category><![CDATA[Statistics]]></category><category><![CDATA[Presidential Campaign]]></category><dc:creator><![CDATA[Quentin Simms]]></dc:creator><pubDate>Thu, 16 Jun 2016 20:31:00 GMT</pubDate><media:content url="/content/images/2016/06/Picture1-1.png" medium="image"/><content:encoded><![CDATA[<h3 id="forgetabouttheweathersohowaboutthattrumpguyhasbecomethepreferredwayofstartingsmalltalkforpeoplearoundtheglobe">Forget about the weather, "So, how about that Trump guy?" has become the preferred way of starting smalltalk for people around the globe.</h3>

<img src="/content/images/2016/06/Picture1-1.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"><p>The US Presidential Primaries have been the world’s biggest news sensation for the past 12 months, and the coverage will only increase now that both major parties have their presumptive nominees for the general election. <strong>Twitter is one of the most powerful media platforms</strong> used by news stations and politicians alike, so <strong>I used it to explore just how many, and what kind, of political messages were being seen by the public</strong> during the weeks and months leading to the end of the presidential primaries. </p>

<p>I learned that the two presumptive nominees, <strong>Donald Trump</strong> and <strong>Hillary Clinton</strong>, were <strong>both mentioned more than the candidates that they were running against in the media</strong>.</p>

<p>I also learned that they were <strong>both polarizing</strong> compared to the other candidates: Their tweets were <strong>more likely to be highly emotional</strong> (either negatively or positively), and <strong>so were the media's tweets about them.</strong></p>

<h1 id="what27000tweetscantellyouaboutthepresidentialrace">What 27,000 Tweets can tell you about the Presidential Race</h1>

<p>In total I collected and analyzed 27,000 tweets by combining the powerful graphical web scraping tool ParseHub and the sentiment analysis API from <a href="http://text-processing.com">text-processing.com</a> with python and <a href="http://jupyter.org">Jupyter Notebook</a>.</p>

<p>My data came from the twitter accounts of the USA’s biggest news sources, as well as from the candidates themselves, but <strong>my process can be used to analyze any twitter account</strong> or social media page – exactly what some software companies may <strong>charge you thousands of dollars per month to do</strong>. In this article I will give you a few tell-tale statistics about the phenomenally popular presidential race, and then I will walk you through how I found them so you that you too can begin to analyze the text data from social media.</p>

<h2 id="themediawasabsolutelyobsessedwiththeprimarycampaigns">The media was absolutely obsessed with the primary campaigns.</h2>

<p>How obsessed? In order to find out, I counted every time a tweet mentioned the name of one of the candidates, from each of the 3 major news networks’ last 3000 tweets.</p>

<p>CNN, Fox News and MSNBC all have general news twitter accounts that they use to share their important stories. The results for these accounts varied by network. On the very lowest end of the scale, <strong>only 22% of @cnn’s last 3000 tweets mention a primary candidate</strong>, while <strong>over <em>half</em> of @msnbc’s last 3000 tweets do</strong>. Considering this account is used to share every important MSNBC story, this is higher than I ever would have expected. It doesn’t matter where you got your news from, you were hearing about the presidential primaries constantly – and possibly even once out every two stories.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/Coverage-from-the-6-News-Sources.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p>The accounts that each network uses to share purely their political stories, @cnnpolitics, @foxnewspolitics and @NBCpolitics, shared very nearly the same number of tweets mentioning the name of a candidate, about 1800 out of 3000, or just under two-thirds of their total tweets. </p>

<h2 id="donaldtrumpwasthefocusoftheattention">Donald Trump was the focus of the Attention.</h2>

<p>Donald Trump was mentioned much more than any of the other 5 other longest running Democratic and Republican candidates. In fact, between May 10 and May 30, all six of the accounts <strong>mentioned him in between 50% and 60% of the tweets</strong> where a candidate was mentioned. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/Picture1.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p>Out of the 17623 tweets I gathered from the 6 different accounts, Donald Trump’s was mentioned in 5035, <strong>close to 1 in every 3 tweets</strong>.</p>

<p>You can see that the two CNN accounts continued to report on the three former Republican candidates, Ted Cruz, John Kasich and Marco Rubio, even after they dropped out, whereas @msnbc and @foxnews stopped reporting on them almost entirely.</p>

<p>We can see how much more attention Trump got over his opponents while they were still running by looking at the tweets of @NBCnewsplotics and @foxnewspolitics, whose tweets continue into late 2015. In the plots below, you can see that the number of times Trump was mentioned from both twitter accounts <strong>grew much faster</strong> than his opponents starting in January, until he was receiving many times more mentions per day than any of the others by mid-March. This is when the other candidates were at the make or break moments in their campaigns.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/Picture2.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/Picture3.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p>The breakdown of how many times each candidate was mentioned between February and May can be found below, and it does look more evenly distributed the post-May bar graph. However, for both networks, Trump nevertheless <strong>received more mentions than all three of his opponents combined.</strong></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/Picture4.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p>It is hard not to speculate that this contributed to his political popularity. Whereas the news stations may be reporting on him because of his widely publicized scandals, they spread awareness of his campaign far more than they have spread awareness of any of the other candidates.</p>

<h2 id="acandidatesmediaattentionisagoodmeasureoftheirpoliticalpopularity">A candidate’s media attention is a good measure of their political popularity.</h2>

<p>So Donald Trump received much more news coverage than his opponents did, right before he secured his position as the Republican presumptive nominee. Was the same true for Clinton, who just secured her position over Sanders on Wednesday night?</p>

<p>The trends are not <em>entirely</em> similar.</p>

<p>On one hand, <strong>The Fox accounts do mention Hillary Clinton much more</strong> than the two other networks, and they do <strong>also mention Bernie Sanders far less</strong>. On the other hand, Sanders was mentioned the most by @msnbc, <strong>nearly equal to the number of times Clinton was mentioned.</strong> The network’s corresponding political account also had coverage that was similar for both candidates but, again, not quite equal. </p>

<p>You can see in the plots below that the coverage on Sanders and Clinton by @msnbc and @NBCpolitics are very similar. For @NBCpolitics specifically, which has tweets dating back to late 2015, you can see that both candidates received similar growth in attention starting in late January.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/MSNBC-Democratic-Mentions.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/NBC-politics-Dems.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p>If you look at the plots of the two Fox News accounts, however, you can see that <strong>Clinton always gets more attention than Sanders.</strong></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/fox-dems.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/foxpols-dems.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p>Unlike the trend seen in the Republican race, <strong>neither network ramped up their coverage on Clinton more than their coverage on Sanders</strong> in the month or two prior to his defeat. When Clinton's attention grew, Sanders' attention grew by about the same amount. Clinton did receive more total mentions, though. Noticing that Trump also secured his party’s nomination when he received the majority of his party’s media attention could be a correlation that would allow Clinton’s win, as well as other future wins, to be predicted.</p>

<p>I will not argue that media attention is the cause for a candidate’s popularity, because it seems even more likely that a candidate’s popularity is the reason for media attention, but both certainly have an effect on each other. </p>

<h2 id="exploringsentiment">Exploring Sentiment</h2>

<p>The number of times someone is mentioned in the news does not necessarily tell you if they’re being positively or negatively portrayed. Even if the news is presented in a neutral and unbiased tone, reporters can easily pick and choose what they want the public to see: for example, it is possible to continually publish quotations from a candidate saying something nasty in order to portray that person as perpetually rotten. Just as easily, they could publish nice words that other people say about a certain candidate for the opposite effect. This is what I hoped to pick up on in my sentiment analysis.</p>

<p>Below you will find the sentiment in every tweet mentioning one of the three candidates. You can see that @msnbc and @NBCpolitics tweets were <strong>slightly more likely to be negative when mentioning Donald Trump, and slightly more positive when mentioning one of the two Democratic Party candidates</strong>. However, they were also consistently more likely to be neutral than either the CNN or or the Fox News accounts.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/hey1.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/hey2.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p>I expected that Fox would show more negative sentiment when mentioning Bernie Sanders, based on the <a href="http://www.journalism.org/2014/10/21/political-polarization-media-habits/">public’s perception</a> that they cater to a <a href="http://www.realclearpolitics.com/articles/2007/03/fox_john_edwards_and_the_two_a.html">conservative audience</a>. This turned out to be the complete opposite of the truth: <strong>for both @foxnews and @foxnewspolitics, Donald Trump received the most negative sentiment</strong>, followed by Hillary Clinton and then <strong>Bernie Sanders with less negative sentiment</strong> than either. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/hey3.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/hey4.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p>@cnn and @cnnpolitics was more likely to be positive when mentioning Hillary Clinton than the other two candidates, though all three politicians received significant amount of positive sentiment from @cnn.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/hey5-1.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/hey6.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<h2 id="trumpstwitterisntasnegativeasitseems">Trump’s twitter isn’t as negative as it seems.</h2>

<p>For those that don’t know, Trump has received criticism for his recurring insults, both in debates and on twitter. The sentiment analysis tool detected that he did, in fact have the <strong>highest percentage of negative tweets</strong>. The most surprising result, as you can see from the graph below, is that <strong>Trump also has the greatest percentage of tweets that registered as positive</strong>.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/a1.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<h2 id="trumpstwitterisntasnegativeasitseemsunlesshestalkingabouttheothercandidates">Trump’s twitter isn’t as negative as it seems: unless he’s talking about the other candidates.</h2>

<p>The story changes when only the tweets mentioning his political rivals are analyzed. Like I mentioned earlier, Donald Trump, the only remaining Republican Party candidate, is known to regularly hurl insults at his opponents. I wanted to see how often he did, and how Clinton and Sanders compared.</p>

<p>Hillary Clinton talked about Donald Trump far more often than she talked about her direct opponent, Bernie Sanders. <strong>When she mentioned Trump, her tweet was more often negative than positive</strong>. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/Clinton-Sentiment.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p>Meanwhile, Sanders mentioned Clinton far more often than he mentioned Trump during his campaign. As seen before, his tweets were more likely to have a predominantly neutral sentiment, but his tweets that mention Clinton were <em>almost</em> as likely to have a negative sentiment than a positive one.</p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/Sanders-Sentiment.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<p>Last but not least, Trump was the <strong>only candidate who was consistently more often negative than positive when mentioning the other candidates</strong>: you can see in the graph below that this is true for all five of his political opponents. </p>

<p><img src="https://www.parsehub.com/blog/content/images/2016/06/Trump-s-Sentiment.png" alt="Twitter predicted the results of the Presidential Primaries. Could it predict the general election, too?"></p>

<h1 id="howcantwitterpredicttheresultsofthegeneralelection">How can twitter predict the results of the general election?</h1>

<p>It will be interesting to see which of these trends continues throughout the general election. Based on what I found during the primary race, <strong>monitoring the candidate’s media attention could be a good way of predicting a winner</strong>. </p>

<p>Additionally, monitoring the sentiment of the candidate’s tweets, and any tweets mentioning their names, could be another indicator: Sanders’ personal tweets were found to be more often neutral than Clinton or Trump’s, <strong>and his mentions in the media also were also more often more often to be neutral</strong>. On the other hand, Trump and Clinton were more polarizing: They had more positive and negative tweets from their personal accounts, and they also had more positive and negative mentions in the news. It may very well be that <strong>polarization correlates with political popularity</strong>, but we will have to wait to see as the race continues.</p>

<h2 id="howigotmyresults">How I got my results</h2>

<p>I used a website called <a href="https://snapbird.org">snapbird.org</a> that shows the 3000 most recent tweets for each twitter account. I found that snapbird.org had a number of interactive elements that I had to deal with when collecting my data:</p>

<ul>
<li>an ajax pop up window</li>
<li>a separate page for logging in to twitter</li>
<li>text input boxes for my twitter username, password, and the name of the account I want to view</li>
<li>a “load more” button that added more tweets to the bottom of the page, 100 at a time.</li>
</ul>

<p>I decided to use my favorite new webpage scraping tool, ParseHub, since it makes collecting data from interactive webpages easy. I quickly put together a set of instructions that would collect the text and date of every available tweet for any given account. See <a href="https://parsehubhelp.zendesk.com/hc/en-us/categories/202638628-Tutorials">ParseHub’s documentation</a> to see what else it can do.</p>

<p>The <a href="https://text-processing.com">text-processing.com</a> API was easily integrated into my iPython Notebook. I looped through the text of each tweet in the pandas objects and sent them, through an HTTP Post, to the text-processing.com URL. In return I was sent a JSON with two elements which I saved into the pandas objects: the “negative”, “neutral” or “positive” label, as well as the probabilities for each sentiment that the API calculated. To learn more about the text-processing.com API, check out the demonstrations <a href="https://market.mashape.com/japerk/text-processing">here</a>.</p>

<p>You can see the specific ParseHub instructions and the HTTP POSTS that I used in this <strong>step-by-step</strong> <a href="https://www.parsehub.com/blog/mining-tweets-for-sentiment-analysis-part-1/">tutorial</a>. It will <strong>teach you to mine the sentiment of tweets just as I did here</strong>, and give you an idea of how it can be done for other social media sites.</p>

<p>To see the <strong>entirety of my python code and tables of data</strong>, see the <a href="https://www.parsehub.com/blog/mining-tweets-for-sentiment-analysis-part-2/">second part</a> of the tutorial, where I go over the analysis, and the creation of the graphs in this article.</p>]]></content:encoded></item></channel></rss>